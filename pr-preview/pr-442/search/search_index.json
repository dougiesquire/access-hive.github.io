{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#welcome-to-access-hive","title":"Welcome to ACCESS-Hive","text":"ACCESS-Hive is a portal to all documentation relevant to the Australian Community Climate and Earth System Simulator, ACCESS, and the wider ACCESS community. ACCESS-Hive is developed for and by the ACCESS community following an open-source development model."},{"location":"#navigating-access-hive","title":"Navigating ACCESS-Hive","text":"Models Run a Model Model Evaluation Community Resources Community Forum"},{"location":"#about","title":"About","text":"The documentation on Hive is work in progress! <p>The ACCESS-Hive is a community resource that is a work in progress. We\u2019d love to receive your contribution. Please see the contributing guidelines below for how to make contributions to the Hive page content. You can also open an issue highlighting any content you\u2019d like us to provide but aren\u2019t able to contribute yourself.</p>"},{"location":"#contribute-to-access-hive-1","title":"Contribute to ACCESS-Hive 1","text":"Contribute Join the ACCESS-Hive team and have your contributions onboard!"},{"location":"#acknowledgement-of-country","title":"Acknowledgement of Country","text":"<p>We at ACCESS-NRI acknowledge the Traditional Owners of the land on which our research infrastructure and community operate across Australia and pay our respects to Elders past and present. We recognise the thousands of years of accumulated knowledge and deep connection they have with all the Earth systems we simulate.2</p> <p></p> <ol> <li> <p>Image by pch.vector on Freepik\u00a0\u21a9</p> </li> <li> <p>Photo by Ren\u00e9 Riegal on Unsplash \u21a9</p> </li> </ol>"},{"location":"call_contribute/","title":"Call contribute","text":"The documentation on Hive is work in progress! <p>The ACCESS-Hive is a community resource that is a work in progress. We\u2019d love to receive your contribution. Please see the contributing guidelines below for how to make contributions to the Hive page content. You can also open an issue highlighting any content you\u2019d like us to provide but aren\u2019t able to contribute yourself.</p>"},{"location":"about/License/","title":"License","text":"<p>The ACCESS-Hive is made available under the Creative Commons Attribution license. The following is a human-readable summary of (and not a substitute for) the full legal text of the CC BY 4.0 license.</p> <p>You are free:</p> <ul> <li>to Share---copy and redistribute the material in any medium or format</li> <li>to Adapt---remix, transform, and build upon the material</li> </ul> <p>for any purpose, even commercially.</p> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p> <p>Under the following terms:</p> <ul> <li> <p>Attribution---You must give appropriate credit (mentioning that your work is   derived from work that is Copyright \u00a9 ACCESS-NRI and, where practical, linking to https://www.access-nri.org.au/), provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</p> </li> <li> <p>No additional restrictions---You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.  With the understanding that:</p> </li> </ul> <p>Notices:</p> <ul> <li>You do not have to comply with the license for elements of the   material in the public domain or where your use is permitted by an   applicable exception or limitation.</li> <li>No warranties are given. The license may not give you all of the   permissions necessary for your intended use. For example, other   rights such as publicity, privacy, or moral rights may limit how you   use the material.</li> </ul>"},{"location":"about/code_of_conduct/","title":"Code of Conduct","text":"<p>ACCESS-Hive is an open community supported effort. For it to be successful it must be a welcoming and inclusive space so that everyone in the community feels able to contribute. </p> <p>To ensure this is the case users and contributors to ACCESS-Hive and the ACCESS-Hive Forum are required to abide by the ACCESS-NRI code of conduct.</p>"},{"location":"about/contact/","title":"Contact","text":"<p>ACCESS-Hive is an initiative of the The Australian Earth-System Simulator (ACCESS-NRI).</p> <p>ACCESS-Hive is an open community supported effort. The underpinning infrastructure is provided by ACCESS-NRI but much of the content is provided by the ACCESS Community.</p> <p>If there are problems or queries about the content of ACCESS-Hive check if there is already a relevant open issue on the ACCESS-Hive GitHub repository, and open one if not.</p> <p>Check the support page for information on what is supported and by whom.</p> <p>Join the ACCESS-Hive forum and find previous related discussions about the hive or the resources listed here, or start your own and make contacts with your community.</p> <p>Otherwise, contact ACCESS-NRI directly. Full contact details for ACCESS-NRI are available on the ACCESS-NRI website contact page</p>"},{"location":"about/contact/#other-places-where-you-can-find-the-access-nri-team","title":"Other places where you can find the ACCESS-NRI team:","text":"<p>: ACCESS-Hive Forum</p> <p>: ACCESS-NRI GitHub</p> <p>: @ACCESS_NRI twitter</p> <p>: access_nri LinkedIn</p> <p>: access-nri.slack.com</p>"},{"location":"about/policies/","title":"Policies","text":"<ul> <li>Procedures and Practices: Contains documents outlining how ACCESS-NRI will function. These documents describe what users can expect and justifications for the decisions against criteria that are based on the values of the organisation.</li> </ul>"},{"location":"about/user_support/","title":"User support","text":""},{"location":"about/user_support/#how-to-get-help","title":"How to get help","text":"<p>Each entry on ACCESS-Hive links to another web site. There should be information on how to get help on the linked site. If there are no obvious channels for help, or the help is not adequate consider asking for assistance from fellow members of your community on the ACCESS-Hive forum. </p> <p>In the case of ACCESS-NRI supported documentation and software, marked , if there is no information on how to get help, or your query is not appropriate for the support channels provided, please either ask on the ACCESS-Hive forum or contact ACCESS-NRI directly.</p>"},{"location":"community_resources/","title":"Community Resources","text":"<p>In this area of the Hive, we collect content that is not currated by us, but may be helpful for the community. You can contribute to this part of the Hive too!</p> <p>Currently, we provide pointers to the following categories: - Working Groups  - Glossaries - Model Evaluation Links - Training - Events </p>"},{"location":"community_resources/community_data_processing/","title":"Community Processing Data Processing Tools","text":""},{"location":"community_resources/community_data_processing/#tools","title":"Tools","text":""},{"location":"community_resources/community_data_processing/#kerchunk","title":"Kerchunk","text":"<p>Documentation |  Sources</p> <p>Kerchunk is a library that provides a unified way to represent a variety of chunked, compressed data formats (e.g. NetCDF/HDF5, GRIB2, TIFF, \u2026), allowing efficient access to the data from traditional file systems or cloud object storage. It also provides a flexible way to create virtual datasets from multiple files.</p>"},{"location":"community_resources/community_data_processing/#cmor3","title":"CMOR3","text":"<p>Climate Model Output Rewriter Version 3</p> <p>Documentation |  Sources</p> <p>CMOR is used to produce CF-compliant netCDF files. The structure of the files created by CMOR and the metadata they contain fulfill the requirements of many of the climate community\u2019s standard model experiments (which are referred to here as \u201cMIPs\u201d and include, for example, AMIP, PMIP, APE, and IPCC scenario runs).</p>"},{"location":"community_resources/community_data_processing/#xmip","title":"xMIP","text":"<p>Documentation | Tutorial on NCI | Sources</p> <p>This package facilitates the cleaning, organization and interactive analysis of Model Intercomparison Projects (MIPs) within the Pangeo software stack.</p>"},{"location":"community_resources/community_data_processing/#app4-the-access-post-processor","title":"APP4 (The ACCESS Post Processor)","text":"<p>Documentation |  Sources</p> <p>The APP4 is a CMORisation tool designed to convert ACCESS model output to ESGF-compliant formats, primarily for publication to CMIP6. The code was originally built for CMIP5, and was further developed for CMIP6-era activities. Uses CMOR3 and files created with the CMIP6 data request to generate CF-compliant files according to the CMIP6 data standards.</p>"},{"location":"community_resources/community_data_processing/#access-archiver","title":"ACCESS-Archiver","text":"<p>Documentation |  Sources</p> <p>The ACCESS Archiver is designed to archive model output from ACCESS simulations. It's focus is to copy ACCESS model output from its initial location to a secondary location (typically from <code>/scratch</code> to <code>/g/data</code>), while converting UM files to netCDF, compressing MOM/CICE files, and culling restart files to 10-yearly. Saves 50-80% of storage space due to conversion and compression.</p>"},{"location":"community_resources/community_data_processing/#synda","title":"Synda","text":"<p>synda is a command line tool to search and download files from the Earth System Grid Federation (ESGF) archive.</p>"},{"location":"community_resources/community_data_processing/#fluxnetlsm","title":"FluxnetLSM","text":"<p>Citation 1 | Sources</p> <p>R package for post-processing FLUXNET datasets for use in land surface modelling. Performs quality control and data conversion of FLUXNET data and collated site metadata. Supports FLUXNET2015, La Thuile, OzFlux and ICOS data releases.</p>"},{"location":"community_resources/community_data_processing/#metpy","title":"Metpy","text":"<p>https://unidata.github.io/MetPy/latest/examples/formats/index.html</p> <p>Documentation | Sources</p> <p>MetPy is a collection of tools in Python for reading, visualizing, and performing calculations with weather data. MetPy supports Python &gt;= 3.8 and is freely available under a permissive open source license.</p> <p>Format types are: GINI Water Vapor Imagery, NEXRAD Level 3 File, and NEXRAD Level 2 File.</p>"},{"location":"community_resources/community_data_processing/#xskillscore","title":"xskillscore","text":"<p>Documentation | Sources</p> <p>xskillscore is a Python library for computing a wide variety of skill metrics. Its typical application is to verify deterministic and probabilistic forecasts relative to observations.</p>"},{"location":"community_resources/community_data_processing/#analysis-blogposts-and-tutorials","title":"Analysis blogposts and tutorials","text":"<p>Accessing NetCDF and GRIB file collections as cloud-native virtual datasets using Kerchunk, Peter March, Sep 2022</p> <ol> <li> <p>A. M. Ukkola, N. Haughton, M. G. De Kauwe, G. Abramowitz, and A. J. Pitman. Fluxnetlsm r package (v1.0): a community tool for processing fluxnet data for use in land surface modelling. Geoscientific Model Development, 10(9):3379\u20133390, 2017. URL: https://gmd.copernicus.org/articles/10/3379/2017/, doi:10.5194/gmd-10-3379-2017.\u00a0\u21a9</p> </li> </ol>"},{"location":"community_resources/community_med_recipes/","title":"Community Model Evaluation and Diagnostics (MED) Recipe Gallery","text":"<p>We are trying to ingest more and more model evaluation and diagnostics recipes in your currated recipe gallery on this website . While this is a continous effort, this site is intented for a list of model evaluation and diagnostics recipes that are not (yet) ingested but may be interesting for the community :</p> MED Recipe Components Description ESMValTool  (Earth System Model EValuation Tool) Documentation |         Tutorial |         Source Code COSIMA Cookbook / Recipes (Consortium for Ocean-Sea Ice Modelling in Australia) Documentation |         Tutorial |         Source Code |         Recipes iLAMB (international Land Model Benchmarking) Documentation |         Tutorial |         Source Code iOMB (international Ocean Model Benchmarking) Documentation |         Tutorial |         Source Code METPlus (Model Evaluation Tool Plus) Tutorial |         Paper PMP (PCMDI Metrics Package) Documentation |         Source Code climpred  Documentation |         Tutorial |         Source Code |         Paper FREVA (Free Evaluation System Framework) Documentation |         Source Code TECA (Toolkit for Extremes Climate Analysis) Documentation |         Tutorial |         Source Code MONET (Model and ObservatioN Evaluation Toolkit) Documentation |         Tutorial |         Source Code |         Paper LIVVkit (land ice verification &amp; validation toolkit) Documentation |         Tutorial |         Source Code CSET (Convective Scale Evaluation Tool ) Documentation |         Tutorial |         Source Code |          MetPy (Model Evaluation Tool Plus) Tutorial |         Source Code |         Recipes         MetPy is a collection of tools in Python for reading, visualizing, and performing calculations with weather data. MetPy supports Python &gt;= 3.8 and is freely available under a permissive open source license.      Afterburner  Documentation |         Source  Pythia Cookbooks {{ community }} Documentation |         Source"},{"location":"community_resources/community_model_catalogs/","title":"Community Model Data Catalogs","text":"<p>We are trying to ingest more and more model data catalogs in your currated catalog on this website. While this is a continous effort, this site is intented for a list of additional model data catalogs that are not (yet) ingested but are recommended by us () or may be interesting for the community ():</p> Model Catalog Comp. Description NCI datasets          NCI has an extensive catalog of datasets of interest to the weather and climate community. These datasets are directly available on the NCI supercomputer and the [Australian Research Environment](https://opus.nci.org.au/display/Help/ARE+User+Guide)          CLEX NCI Data Collection Intake Catalogue          This is an Intake catalogue maintained by the ARC Centre of Excellence for Climate Extremes [(CLEX)](https://climateextremes.org.au/).         Only datasets from the NCI Catalog are referenced.         The catalogue is available in intake's default catalogue list in the CLEX Conda environment.         Two notebooks are provided in the docs folder showing how to access the ERA5 and CIP6 datasets.          Australia Climate Data Guide Catalogue          *A one-stop catalogue to discover Climate Data in Australia*         The ACDG portal is a metadata portal listing climate research resources available in Australia from multiple data repositories.         This is a community based project managed by the ACDG Single Access working group. This is a group of Australian climate community self-nominated representatives. Anyone is welcome to join the group or to contribute independently to the metadata portal the group is developing.          Australian Ocean Data Network          The Australian Ocean Data Network (AODN) is an interoperable online network of marine and climate data resources.  IMOS and the 6 Australian Commonwealth agencies ([see AODN Partners](https://imos.org.au/facilities/aodn/aodn-data-management/aodn-partners)) form the core of the AODN. Increasingly, though, universities and State government offices are offering up data resources to the AODN, and delivery of data to the AODN is being written in to significant research programs e.g. [National Environmental Science Program Marine Biodiversity Hub](http://www.nespmarine.edu.au/) and the [Great Australian Bight research program](http://www.misa.net.au/GAB).          Intake-Ilamb Catalog          The Intake-Ilamb catalog provides an yaml-style intake catalog of the reference data used for ESM model benchmarking in the International Land Model Benchmarking [(ILAMB)](https://www.ilamb.org/) effort.          FLUXNET          FLUXNET is an international \u201cnetwork of networks,\u201d tying together regional networks of earth system scientists. FLUXNET scientists use the eddy covariance technique to measure the cycling of carbon, water, and energy between the biosphere and atmosphere. Scientists use these data to better understand ecosystem functioning, and to detect trends in climate, greenhouse gases, and air pollution.          CEDA Archive          The CEDA Archive forms part of NERC's Environmental Data Service (EDS) and is responsible for looking after data from atmospheric and earth observation research. They host over 18 Petabytes of data from climate models, satellites, aircraft, met observations, and other sources.          OZFlux          OzFlux is an ecosystem research network set up to provide Australian, New Zealand and global ecosystem modelling communities with consistent observations of energy, carbon and water exchange between the atmosphere and key Australian and New Zealand ecosystems.          Australian Community Reference Climate Data Collection          This collection is a collaborative effort between the Australian Climate Service (ACS), ARC Centre of Excellence for Climate Extremes (CLEX) and the wider Australian climate research community to re-establish and maintain a reference dataset collection at NCI.         An [intake-esm catalogue](https://github.com/aus-ref-clim-data-nci/acs-replica-intake) is also available to facilitate data access."},{"location":"community_resources/community_observational_catalogs/","title":"Community Observational Data Catalogs","text":"<p>We are trying to ingest more and more model data catalogs in your currated catalog on this website. While this is a continous effort, this site is intented for a list of additional model data catalogs that are not (yet) ingested but are recommended by us () or may be interesting for the community ():</p> Data Catalog Comp. Description Copernicus Climate Change Service (C3S) Data Store (CDS)          The Copernicus Climate Change Service (C3S) combines observations of the climate system with the latest science to develop authoritative, quality-assured information about the past, current and future states of the climate in Europe and worldwide. C3S data is provided via its Climate Data Store (CDS).         You can search its available datasets via this interface.         You can use the CDS API as well as command line tools to download data. To download ERA5 from CDS, you can use for example this era5cli command line tool.          Catalogue Search at CEDA (Centre for Environmental Data Analysis)           The CEDA (Centre for Environmental Data Analysis) Archive hosts atmospheric and earth observation data.         It provids an interactive Catalogue Search and Tools for downloading data.         It holds environmental data related to atmospheric and earth observation fields. Our remit covers the following areas (see linked examples to some of our most popular datasets):          - Climate - e.g. HadUK Grid, CMIP, CRU          - Composition - e.g. CCI          - Observations - e.g. MIDAS Open          - Numerical weather prediction - e.g. Met Office NWP          - Airborne - e.g. FAAM          - Satellite data and imagery - e.g. Sentinel"},{"location":"community_resources/community_working_groups/","title":"Community Working Groups","text":"<p>The ACCESS Community and the ACCESS-NRI have established Community Working Groups to assess and prioritise the needs of the modelling community as well as encourage collaboration within. These working groups are open to the community and welcome new members.</p> <p>The working group activities are coordinated through the ACCESS Hive Community Forum:</p>              Atmospheric Modelling                       Land Surface Modelling                       COSIMA(Ocean and Sea-Ice)                       Forecasting and Prediction                       Earth System Modelling                       Cryosphere          <p>To join a working group follow the instructions on the ACCESS-NRI website, and to participate in the activities of the working group visit the ACCESS-hive forum.</p>"},{"location":"community_resources/events/","title":"Workshops and Conferences","text":"<p>{{ events_content }}</p>"},{"location":"community_resources/events/add_event/","title":"Workshops and Conferences: Add Event","text":"<p>We encourage members of the community to list any workshops, tutorials, conferences that might be of interest to the community.</p>"},{"location":"community_resources/events/add_event/#how-to-add-your-event","title":"How to add your event","text":""},{"location":"community_resources/events/add_event/#add-an-issue","title":"Add an issue","text":"<p>The easiest way for you to add your event is to make an issue with the template provided. This provides a form which guides you through the process of providing the required information.</p>"},{"location":"community_resources/events/add_event/#create-a-pull-request-to-add-your-event","title":"Create a pull-request to add your event","text":"<p>This process requires some knowledge of git, GitHub and Markdown. If you do not feel comfortable doing this then it is sufficient to just add an issue as above. The issue will be assigned to someone else to finish.</p> <p>If you do feel confident adding your event to the list, then create a Markdown text file, identified with the <code>.md</code> extension, to the correct subdirectory in the <code>events</code> folder of the ACCESS-Hive repository. The subdirectories are named by year, put your new file in the year in which the event will take place. Avoid spaces in your filename: use an underscore <code>_</code> where you would normally have a space. e.g. <code>regional_dowscaling_cordex.md</code></p> <p>The file must contain a header with the metadata as in the example below:</p> <pre><code>---\ntitle: Regional climate downscaling for Australia within the CORDEX framework\nstart_date: 27/11/2022\nend_date: 27/11/2022\nlocation: Adelaide, SA\nlink: https://www.amos2022.org.au/\ndescription: This workshop is relevant for those performing regional climate simulations or downscaling with empirical/statistical downscaling approaches including machine learning, as well as those using regional climate projection data in their work. The focus will be on CORDEX related data and modelling. The workshop will have some presentations with extended discussion.\n---\n</code></pre> <p>Make sure to follow all the steps described in the contribution guidelines to submit this addition for approval for publication.</p>"},{"location":"community_resources/events/events/2022/CORDEXAmos2022/","title":"Regional climate downscaling for Australia within the CORDEX framework","text":"<p>This workshop is relevant for those performing regional climate simulations or downscaling with empirical/statistical downscaling approaches including machine learning, as well as those using regional climate projection data in their work. The focus will be on CORDEX related data and modelling. The workshop will have some presentations with extended discussion. Some topics to be covered include: - Accessing the existing CORDEX-CMIP5 data. How to access and use the data - Explain the CORDEX-CMIP6 protocol - What does it say? How can you contribute? - Who is planning to contribute (or is already working on contributions) to the Australasia domain?</p>"},{"location":"community_resources/events/events/2022/GC5Workshop/","title":"GC5 Assessment Workshop","text":"<p>The UM Partnership Team and GC Programme Team are running a GC5 assessment workshop, to assess the latest configuration of the Global Coupled model.</p> <p>The workshop will be a hybrid event with an option to attend online or in-person at Met Office Collaboration Building, Exeter. We will discuss the assessment of the latest GC5 configuration in a range of model simulations in a seamless context and sessions will broadly consist of: - Summary of GC5 physics changes - General model assessment - Summary from Priority Evaluation Groups (PEGs) - Summary from Collaboration Groups (CoGs) - Upcoming changes in GC science and tools - Discussions</p> <p>Please fill in the registration form to confirm attendance by 21st October</p> <p>For any further questions please contact Luke Roberts, Prince Xavier or Charline Marzin at the Met Office.</p>"},{"location":"community_resources/events/events/2022/GroundTruthingClimateChange/","title":"Ground truthing future climate change","text":"<p>Scientific ocean drilling provides the robust baseline data on global climate evolution over extended geologic time periods that are critical for improving climate model performance. By targeting how the climate system operates across a wide array of past climate states, scientific ocean drilling has, and continues to, obtain the data necessary to calibrate and improve numerical models used to project future climate impacts and inform mitigation strategies.</p> <p>Join us in this session where we aim to connect climate and ocean modellers to our rich (50+ years of drilling) database and unanswered questions in scientific ocean drilling. By addressing key questions about Earth\u2019s past, present, and future through interdisciplinary research, we are aiming to spark new collaborations and proposals that will lead to a more profound understanding of Earth as one integrated, interconnected system.</p>"},{"location":"community_resources/events/events/2022/SWOTAmos2022/","title":"The Surface Water and Ocean Topography (SWOT) satellite: A primer","text":"<p>The Surface Water and Ocean Topography (SWOT) satellite, which will launch in November 2022, is a ground-breaking wide-swath altimetry mission that will observe fine details of the ocean dynamics at a resolution 10 times finer than current satellites. SWOT is jointly developed by NASA and CNES with contributions from researchers around the world, including Australia. The Australian government, the Integrated Marine Observing System, and the Australian marine science community are investing in SWOT through calibration/validation and synergistic in situ measurements of fine-scale ocean dynamics in the Australian region. This workshop will present a primer on the principles of the satellite and instrument, how it works, and what are its possibilities and limitations compared to existing altimetry products. This will be complemented with a brief summary of ocean research related to and enabled by SWOT, including internal waves and tides, sub-mesoscale dynamics, the geoid, and mean dynamic topography. The goal of the workshop is to provide oceanographers, hydrologists and other users of altimetry data with the information they need to prepare for the arrival of SWOT data in late 2023.</p>"},{"location":"community_resources/glossaries/","title":"Glossary and Useful Terms","text":"<p>Here you can find a compilation of common terms and acronyms used by the Australian Community Climate and Earth System Simulator (ACCESS)  community:</p> <p>ACCESS-NRI Glossary: This includes common terms and acronyms used by the ACCESS community in research and modelling of past, present and future climate, weather and Earth-Systems.</p> <p>IPCC Acronyms: Contains a comprehensive list of acronyms published in the scientific report SR15 of the Intergovernmental Panel on Climate Change (IPCC). The IPCC is the United Nations body for assessing the science related to climate change.</p> <p>CSSR Acronyms and Units: The Climate Science Special Report (CSSR) is an assessment of the science of climate change with particular focus on the United States.</p>"},{"location":"community_resources/training/","title":"Training and Policies","text":"<p>This space is intended to promote training material relevant to ACCESS and its community. The training material can be directly relevant to ACCESS and its model components, such as:</p> <ul> <li>using coupled models and model components</li> <li>using configurations</li> <li>using model evaluation tools and workflows</li> </ul> <p>It is also intended for training material around more peripheral topics that are essential for the community, such as:</p> <ul> <li>HPC</li> <li>version control</li> <li>essential software packages</li> </ul> <p>ACCESS-NRI encourages the members of the community to contact us to share their suggestions.</p> <p>Finally, you will also find ACCESS-NRI's policies in this space.</p>"},{"location":"community_resources/training/ACCESS_training/","title":"ACCESS Training Material","text":"<p>This page is intended to provide access to training material directly related to ACCESS models and model components. This material can cover topics such as but not limited to:</p> <ul> <li>how to use a specific model</li> <li>how to modify a model configuration</li> <li>how to test a model modification or validate a model run</li> </ul>"},{"location":"community_resources/training/ACCESS_training/#jules-tutorials","title":"JULES tutorials","text":"<p>The JULES tutorials explain how to use FCM, Rose and Cylc both for using the model and for development work. They can be useful to ACCESS users as practical demonstrations of the Rose and Cylc infrastructure.</p>"},{"location":"community_resources/training/additional_training/","title":"Additional training material","text":"<p>To learn the basics of Git and GitHub. It also includes ACCESS-NRI's recommendations to setup GitHub. </p> <p></p> <p>The National Computational Infrastructure (NCI) provides training resources and in-person training courses throughout the year to help develop the skills of the NCI user community.  </p> <p>A full calendar of upcoming training opportunities can be found on their Opus page.</p> <p>Users can find important information and resources about using NCI systems and services in the NCI User Guides.</p>"},{"location":"contribute/","title":"How to Contribute","text":"<p>ACCESS-Hive is a community supported site, as such contributions to the ACCESS-Hive site are encouraged by any member of the community. Members of the ACCESS community are also welcome to become reviewers. Please refer to the following contribution guidelines to learn how you can help the ACCESS community build a documentation database useful to everyone.</p> <p>Although we encourage everyone to get involved and contribute to the ACCESS-Hive in order to adequately represent the needs of the entire ACCESS community, we recognise not everyone will have the time to do so. In case you do not have a lot of time, please consider sharing your ideas via issues on the ACCESS-Hive GitHub repository so someone might be able to add them to the ACCESS-Hive site for you.1</p> Abstract <p>The aim of this how-to is to enable you to:</p> <ul> <li>add or modify a link to a new documentation in an existing page</li> <li>contribute complex modifications, eg., add pages, modify the navigation, modify an existing page in depth etc.</li> <li>how to deal with relevant documentation that is not currently on a website</li> </ul>"},{"location":"contribute/#become-a-member-of-the-access-hive-organisation","title":"Become a member of the ACCESS-Hive organisation","text":"<p>The ACCESS-Hive organisation is open to any member of the ACCESS community. Furthermore, organisation members have write access to the ACCESS-Hive repository which simplifies the process to contribute. Members can work from branches that contain their modifications instead of creating and maintaining their own forks. </p> <p>As such, we encourage you to become a member of the organisation by replying to this issue and ask to be invited to join the organisation.</p>"},{"location":"contribute/#process-to-contribute","title":"Process to contribute","text":"<p>This documentation is based on the Material for Mkdocs theme. Please see the documentation for the theme or for Mkdocs for a full explanation of all the capabilities.</p> <p>The documentation is written in Markdown format. Please see this cheat sheet for a quick reference to the base syntax. Please note that Material for Mkdocs extends that syntax.</p> <p>Additionally, ACCESS-Hive is a portal for documentation hosted elsewhere. The documentation you want to add needs to be available from an existing website. We realise people might have standalone files or other information to share, please see our Standalone documentation page for ways to easily upload your documentation to a site.</p> <p>There are two main ways to contribute to the site:</p> <ul> <li>you can modify an existing page directly from GitHub without any knowledge of Git. This is a simple way suitable to light modifications.</li> <li>you can work on your local computer and use Git to manage your modifications. This is recommended for more involved modifications. It is the easiest way to modify the categories and menu structure used to navigate the site.</li> </ul> <ol> <li> <p>Image by pch.vector on Freepik\u00a0\u21a9</p> </li> </ol>"},{"location":"contribute/change-the-navigation/","title":"Change the navigation","text":""},{"location":"contribute/change-the-navigation/#structure-of-the-repository","title":"Structure of the repository","text":"<p>The important elements of the repository to know about before modifying the navigation are:</p> <ul> <li><code>docs/</code> folder: this folder contains all the documentation pages. There is an <code>index.md</code> file for the About page, one folder per tab on the site, an <code>assets/</code> folder to store images used in the documentation and some customisation folders such as <code>css/</code> or <code>font/</code>.</li> <li><code>mkdocs.yml</code>: it is a YAML formatted file, hence the <code>.yml</code> extension. The site navigation is defined in this file as well as options for the styling of the site.</li> </ul> YAML <p>YAML is a popular choice for configuration files, as it is a simple way of encoding data structures in a text file. See this short tutorial.</p>"},{"location":"contribute/change-the-navigation/#a-simple-example","title":"A simple example","text":"<p>The easiest way to explain how the navigation is defined is to look at an example. Let's say <code>mkdocs.yml</code> contains:</p> <pre><code>nav:\n- Welcome: index.md\n- ACCESS-NRI: ACCESS-NRI/ACCESS-NRI.md\n- Community: - Generate Bathymetry: community/bathymetry.md\n- How to contribute: - How to contribute: help/how_to_contribute.md\n- Setup: help/contribution_setup.md\n- Modify the documentation: help/modify_documentation.md\n- Change the navigation: help/change_navigation.md\n</code></pre> <p>The top level category names define the tabs in the header bar. So here we have the tabs: \"Welcome\", \"ACCESS-NRI\", \"Community\" and \"How to contribute\". It is also the name of the top section under each tab.</p> <p></p> <p>The second level of categories indicate the name of each page under that section. So the \"ACCESS-NRI\" tab has the text directly under the section \"ACCESS-NRI\". The \"Community\" tab has a section called \"Community\" that contains one page: \"Generate Bathymetry\". Finally, the \"How to contribute\" tab has 1 section \"How to contribute\" with 4 pages.</p> <p>The filenames indicate the path to the file relative to the <code>docs/</code> folder containing the text for each page. It is recommended to use the title of each file (i.e. the heading level 1) or an abbreviation of it as the name of the page and the filename.</p> <p></p>"},{"location":"contribute/change-the-navigation/#add-sections-to-a-tab","title":"Add sections to a tab","text":"<p>It is possible to define several sections per tab by using more levels of indentation. For example, to add a \"My example\" section to the \"How to contribute\" tab:</p> <pre><code>nav:\n- Welcome: index.md\n- ACCESS-NRI: ACCESS-NRI/ACCESS-NRI.md\n- Community: - Generate Bathymetry: community/bathymetry.md\n- How to contribute: - How to contribute: help/how_to_contribute.md\n- Setup: help/contribution_setup.md\n- Modify the documentation: help/modify_documentation.md\n- Change the navigation: help/change_navigation.md\n- My example:\n- Beautiful example: help/beautiful_example.md\n</code></pre> <p>will create this navigation: </p>"},{"location":"contribute/edit-locally/","title":"Edit locally on your computer","text":"<p>If you want to submit a substantial contribution to ACCESS-Hive, it might be easier to do so from your own computer. Especially, it is a lot easier to proceed locally if you need to modify several files or want to modify the navigation of the site.</p> <p>You can avoid creating your own fork for the repository by first becoming a member of ACCESS-Hive organisation. To become a member, please reply to this issue and ask to be invited to join the organisation.</p> <p>To work locally, you will need git and a text editor installed on your computer.</p>"},{"location":"contribute/edit-locally/#open-an-issue","title":"Open an issue","text":"<p>For all additions or modifications to the ACCESS-Hive site, it is recommended to start by opening an Issue in the ACCESS-Hive GitHub repository. Consider assigning the Issue to yourself in the right-hand side panel if you intend on working on the issue and you are a member of the ACCESS-Hive organisation.</p>"},{"location":"contribute/edit-locally/#obtaining-the-source-files","title":"Obtaining the source files","text":"<p>Everything is stored within the ACCESS-Hive repository on GitHub and you simply need to clone this repository to your local machine:</p> <pre><code>git clone git@github.com:ACCESS-Hive/access-hive.github.io.git\n</code></pre>"},{"location":"contribute/edit-locally/#edit-to-access-hive","title":"Edit to ACCESS-Hive","text":"<p>Once you have installed all you need, you will need to follow the usual series of steps when contributing to Open Source developments:</p> <ul> <li>open an Issue</li> <li>clone the repository locally</li> <li>start a branch to work on, linked to the Issue</li> <li>commit your modifications to that branch and push to GitHub</li> <li>open a pull request between the <code>main</code> branch and your branch, follow the instructions from the Pull request template.</li> <li>ask for reviews by tagging the ACCESS-Hive/reviewers team and reply to requests for changes</li> </ul> <p>If you don't know how to do these steps, please refer to our Git and GitHub training.</p> What page to edit <p>If you have problems finding the page you need to edit, the easiest way is to head to the ACCESS-Hive site. If you click on the pen icon  at the top right of each page title, you will open a GitHub page showing you the path to the file you want to edit.</p> Headers and table of content <p>The level 1 headers are reserved for the title of the page and are ignored from the pages' table of contents. Only use level 2 headers and higher to organise pages.</p>"},{"location":"contribute/edit-locally/#add-a-new-event","title":"Add a new event","text":"<p>The process to add a new event is a bit different from other updates on the site. Since you need to create a new file to contain the information about the event you are adding, it is recommended to work locally. You need to create a new Markdown file (identified by the <code>.md</code> extension) as described on this page. To record and submit your modification to the site, make sure you follow all the steps as explained in the Open Source process in the previous section.</p>"},{"location":"contribute/edit-locally/#preview-of-the-documentation","title":"Preview of the documentation","text":""},{"location":"contribute/edit-locally/#preview-from-a-pull-request","title":"Preview from a Pull Request","text":"<p>When a pull request is created or updated, GitHub will automatically build a preview of the documentation that includes the proposed changes.</p> <p>In the pull request, you will see the link to the preview appear in this fashion:</p> <p></p> Build delay <p>It can take a while for the preview to build, even after the CI check is indicated as finished. Please wait for the comment with the link to appear and allow for some time after that for the preview to be properly deployed.</p> <p>If you open the preview and it looks completely broken or if it hasn't updated from additional modifications in the pull request, it probably means the site hasn't finished building yet. If you wait a couple of minutes and refresh the page, it should fix itself.</p>"},{"location":"contribute/edit-locally/#local-preview-if-editing-on-your-own-computer","title":"Local preview (if editing on your own computer)","text":"<p>MkDocs includes a live preview server, so you can preview your changes as you write your documentation. The server will automatically rebuild the site upon saving.</p>"},{"location":"contribute/edit-locally/#software-installation","title":"Software installation","text":"<p>To build the site locally, you need to install [Material for Mkdocs][MatforMkdocs] and other plugins. You can find a full list in the <code>requirements.txt</code> file in the root of the ACCESS-Hive repository. Please use <code>pip</code> for the installation as some of packages are not updated or not available on <code>conda</code>:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"contribute/edit-locally/#start-the-server","title":"Start the server","text":"<p>To start the server, open a terminal and navigate to your ACCESS-Hive local repository. Now type:</p> <pre><code>mkdocs serve\n</code></pre> <p>Your documentation will be built on http://127.0.0.1:8000. Open this URL in your browser to see a preview of the documentation. The URL is given in the terminal when running the <code>mkdocs serve</code> command. Make sure you keep the command running so as to see live updates on saving your modifications.</p>"},{"location":"contribute/edit-on-github/","title":"Edit directly on GitHub","text":"<p>This way to edit the site allows people with no knowledge of Git to contribute to ACCESS-Hive but is only suitable for light modifications of existing pages.</p> <ul> <li>Go to the page you want to modify on the ACCESS-Hive documentation site. At the right of the title, you will see a pen icon . When you click on this icon, your browser will open the file in GitHub allowing you to edit the file.</li> </ul> <ul> <li>Enter your modification in the main pane. All the files are written in Markdown.</li> </ul> Headers and table of content <p>The level 1 headers are reserved for the title of the page and are ignored from the pages' table of contents. Only use level 2 headers and higher to organise pages.</p> <ul> <li>Then add a commit message in the Commit changes box.</li> </ul> <ul> <li>Commit and open a pull request</li> </ul> Pull request is required <p>The <code>main</code> branch of the repository is protected and nobody can write to it directly. You will need to choose either to create a new branch (for ACCESS-Hive organisation members only) or to create a fork on your GitHub personal account (for non-members of ACCESS-Hive organisation) and then open a pull request in all cases. </p> <p>When creating the pull request, make sure to follow the instructions given to you in the pull request template. The description can be edited at any time. You can fill in the check list after creating the pull request. The pull request will automatically build a preview of the documentation with your proposed changes.</p> <ul> <li> <p>Ask for a review by tagging the <code>@ACCESS-Hive/reviewers</code> team in a comment.</p> </li> <li> <p>Reply to the review. You will be notified by email of any subsequent comment, request or action from the reviewer on this pull request. Please make sure you take any action required by the reviewer or your modification will not be accepted into the ACCESS-Hive site.</p> </li> </ul>"},{"location":"contribute/edit-on-github/#further-edits","title":"Further edits","text":"<p>During the review process, you might be requested to edit your proposed changes. For this, you will need to navigate to the branch created by GitHub. </p> <ul> <li>At the top of the Pull request window on GitHub, you should see a link to your branch, circled in red on the image:</li> </ul> <ul> <li>Once you click on this link, navigate to and open the file you need to modify, then click on the pen icon in the toolbar on the right, circled in red on the image:</li> </ul> <ul> <li> <p>Then commit your changes once again to the same branch. This will update the pull request and the preview of the site.</p> </li> <li> <p>You need to let the reviewer know once you are confident you have responded to all their concerns, so they can review again. For this, locate the \"Reviewers\" pane in the right-hand side menu list on GitHub and click the icon circled in red in the image:</p> </li> </ul>"},{"location":"contribute/reviewers/","title":"Reviewing for ACCESS-Hive","text":"<p>Any member of the ACCESS-Hive Github Community (to join) can join the reviewers team. Please ask one of the maintainers to invite you to join the reviewers team.</p>"},{"location":"contribute/reviewers/#reviewer-guidelines","title":"Reviewer Guidelines","text":"<p>Firstly, thank you so much for helping to review links submitted to the ACCESS-Hive, we\u2019re delighted to have your help. This document is designed to outline our editorial guidelines and help you understand our requirements for accepting a pull request (PR).</p>"},{"location":"contribute/reviewers/#guiding-principles","title":"Guiding principles","text":"<p>If the submitting authors have followed the contribution guidelines then the review should be rapid. An important requirement is the ACCESS-Hive is a portal to documentation, it does not host the documentation itself.</p> <p>For those PRs that don\u2019t quite meet the requirements, please try to give clear feedback on what needs fixing. Our goal is to maintain a high quality platform for exchanging links to relevant documentation and you, as a reviewer, have a key role to play.</p> <p>A review involves checking submissions against a checklist of essential features and details described at the top of each PR. This should be objective, not subjective; it should be based on the materials in the submission as perceived without distortion by personal feelings, prejudices, or interpretations.</p> <p>Some continuous tests such as hyperlink references checks and preview deployments are automatically triggered by submitting a PR.</p> <p>Reviewers should:</p> <ol> <li>Ensure that the tests are passing without errors.</li> <li>Do a visual check using the preview.</li> <li>Do a Github pull request review. See GitHub's extensive documentation</li> <li>Once you have approved the PR. Tag the editors team <code>@ACCESS-Hive/editors</code> in the discussion.</li> </ol> <p>We encourage reviewers to provide feedback from within the PR discussion.</p> <p>You can include in your review links to any new issues that you the reviewer believe to be impeding the acceptance of the pull request.</p>"},{"location":"contribute/standalone-documentation/","title":"Standalone documentation","text":"<p>You may have very valuable resources to share which are not currently available through any website, it is what we call \"standalone documentation\" in the current context.</p> <p>To contribute these resources to ACCESS-Hive, you will first need to make them available on the internet. Below are some ideas on how to do that:</p> <ol> <li>Check if your organisation has a documentation or information site suitable for your resource.</li> <li>Check if ACCESS-NRI has a documentation site suitable for your resource. Note in this case, you will be asked about ownership and license associated with your resource.</li> <li>Publish your documentation via Zenodo. This will clarify the licensing and reuse conditions.</li> </ol> <p>If none of the previous options seem suitable to you, please consult the forum.</p>"},{"location":"get_started/","title":"Get Started","text":"<p>If you are new to climate science or ACCESS models, and you want to:</p> <ul> <li>Run your own experiment</li> <li>Get model outputs</li> <li>Evaluate model performance</li> <li>Perform other tasks involving ACCESS Models</li> </ul> <p>You will need to follow these steps to get started with any of the tasks above.</p>"},{"location":"get_started/#create-an-nci-user-account","title":"Create an NCI user account","text":"<p>Most of the data and models you will need are available at the National Computing Infrastructure (NCI) . To be able to access them, you need an NCI account.  Sign up here if you don't have one yet.</p>    You will need an institutional email address with an organisation that allows access to NCI (e.g. an Australian university, ACCESS-NRI, CSIRO, CLEX, etc.).      If you don't think you possess an email address with such institution, please get in contact.   <p>Once you sign up, you will be assigned a username (e.g. <code>ab1234</code>). We will also refer to this username as <code>$USER</code>.</p>"},{"location":"get_started/#join-relevant-nci-projects","title":"Join relevant NCI projects","text":"<p>To join a project, search for it on NCI website and request membership.</p> <p>Every project has an ID (e.g. <code>xp65</code>). This ID is what the term project actually refers to.  The first project that you join will become your default one. We will also refer to it as <code>$PROJECT</code>.  If you want to change your default project, please check how to change your default project on gadi.</p> <p>There are several NCI projects that may be relevant to you, depending on the tasks you want to carry out.  Even though we recommend you have a chat with your supervisor to identify the relevant projects for your needs, the table below has a list of some of the most useful climate-related projects at NCI:</p> Project Description Group tm70 ACCESS-NRI Working Project ACCESS-NRI iq82 ACCESS-NRI MED Compute ACCESS-NRI kj13 ACCESS-NRI MED Data Dev ACCESS-NRI ct11 ACCESS-NRI Replicated Datasets ACCESS-NRI xp65 ACCESS-NRI Analysis Environments ACCESS-NRI access ACCESS software sharing ACCESS p66 ACCESS - AOGCM / suppport development of the ACCESS modelling system ACCESS p73 ACCESS Model Output Archive (AOGCM) ACCESS hh5 Climate-LIEF Data Storage Data output ub7 Seasonal Prediction ACCESS-S1 Hindcast Data output ux62 Seasonal Prediction ACCESS-S2 Hindcast Data output cb20 ESGF CMIP3 Replication Data Data output al33 ESGF CMIP5 Replication Data Data output rr3 ESGF CMIP5 Australian Data Publication Data output oi10 ESGF CMIP6 Replication Data Data output fs38 ESGF CMIP6 Australian Data Publication Data output rt52 ERA5 Replicated Data: Single and pressure-levels data Data output uc16 ERA5 Replicated Datasets on Potential Temperature &amp; Vorticity Levels Data output zz93 ERA5-Land Replicated Data Data output zv2 Australian Gridded Climate Data (AGCD) Collection Data output qv56 Reference Datasets for Climate Model Analysis/Forcing Data output cj50 COSIMA Model Output Collection Data output ik11 COSIMA shared working space Other projects v45 Ocean Extremes Other projects ga6 Modelling the formation of sedimentary basins and continental margins Other projects m18 Evolution and dynamics of the Australian lithosphere Other projects q97 Earth dynamics and resources over the last billion years Other projects qu79 Collaborative REAnalysis Technical Environment Intercomparison Project (CREATE-IP) Other projects"},{"location":"get_started/#log-in-to-gadi","title":"Log in to Gadi","text":"<p>Operations such as model runs and output data I/O take place on the Gadi supercomputer.</p> <p>To log in to Gadi you need a few pre-requisites:</p> <ul> <li>Internet connection</li> <li> UNIX-like terminal      Operative Systems such as Linux or MacOS already have a built-in UNIX-like terminal.          Windows users can get    </li>    If you have never logged onto Gadi before, we recommend to take a look at NCI's [Welcome to Gadi website](https://opus.nci.org.au/display/Help/0.+Welcome+to+Gadi). It provides all the important commands and information for logging properly onto Gadi, like the following: \"To run jobs on Gadi, you need to first log in to the system. Users on Mac/Linux can use the built-in terminal. For Windows users, we recommend using [MobaXterm](https://mobaxterm.mobatek.net/) as the local terminal. Logging in to Gadi happens through a Gadi login node.\"  When you login, via the command <pre><code>ssh -Y $USER@gadi.nci.org.au\n</code></pre> you will enter your $HOME directory with your default `$PROJECT` and your default SHELL. Both are saved at `$HOME/.config/gadi-login.conf` and you can print them via <pre><code>cat $HOME/.config/gadi-login.conf\n</code></pre>  The `-Y` option is needed to run graphical tools by enabling the forwarding of trusted X protocol mesgs between X-Server on local system and X programs on Gadi.  You need to enable X Windowing system on your local system before running ssh. This can be done by running X-Server like XQuartz (Mac), MobaXterm (MS Windows), startx or similar (Linux).  Again, for more useful information we recommend to check out NCI's [Welcome to Gadi website](https://opus.nci.org.au/display/Help/0.+Welcome+to+Gadi).  ## 4) Computing on Gadi  ### Gadi Resources Coupled climate models like ACCESS-CM involve, among other things, calculation of complex mathematical equations that explain the physics of the atmosphere and oceans. Performed at hundreds of millions of points around the Earth, these calculations require vast computing power to complete them in a reasonable amount of time, thus relying on the power of  high-performance computing (HPC) like Gadi. The [Gadi supercomputer](https://nci.org.au/our-systems/hpc-systems) can handle more than 10 million billion (10 quadrillion) calculations per second and is connected to 100,000 Terabytes of high-performance research data storage.  An overview of [Gadi resources](https://opus.nci.org.au/display/Help/0.+Welcome+to+Gadi#id-0.WelcometoGadi-GadiResources) such as compute, storage and PBS jobs are described below.   Useful NCI commands to check your available compute resources are:  | Command                |   Purpose                  | | ---------------------- | -------------              | | `logout` or ++ctrl+\"D\"++ | To exit a session          | | `hostname`             | Displays login node details| | `module list`          | Modules currently loaded   | | `module avail`         | Available modules          | | `nci_account -P [proj]`| Compute allocation for [proj]| | `nqstat -P [proj]`     | Jobs running/queued in [proj]| | `lquota`               | Storage allocation and usage for all your projects|  #### Compute Hours Compute allocations are granted to projects instead of directly to users and, hence, you need to be a member of a project in order to use its compute allocation. To run jobs on Gadi, you need to have sufficient allocated [compute hours](https://opus.nci.org.au/display/Help/0.+Welcome+to+Gadi#id-0.WelcometoGadi-ComputeHours) available, where the [job cost](https://opus.nci.org.au/display/Help/2.+Compute+Grant+and+Job+Debiting) depends on the resources reserved for the job and the amount of walltime it uses.   #### Storage  Each user has a project-independent [`$HOME`](https://opus.nci.org.au/display/Help/0.+Welcome+to+Gadi#id-0.WelcometoGadi-TheHomeFolder$HOME) directory, which has a storage limit of 10 GiB. All data on `/home` is backed up.  Through project membership, the user gets access to the storage space within the [project folders](https://opus.nci.org.au/display/Help/0.+Welcome+to+Gadi#id-0.WelcometoGadi-ProjectFolderonLustreFilesystems/scratchand/g/data) `/scratch` and  `/g/data` filesystems for that particular project.  #### PBS Jobs To run compute tasks such as an ACCESS-CM suite on Gadi, users need to submit them as *jobs* to *queues*. Within a [job submission](https://opus.nci.org.au/display/Help/0.+Welcome+to+Gadi#id-0.WelcometoGadi-JobSubmission), you can specify the queue, duration and computational resources needed for your job. When a job submission is accepted, it is assigned a jobID (shown in the return message) that can then be used to monitor the job\u2019s [status](https://opus.nci.org.au/display/Help/0.+Welcome+to+Gadi#id-0.WelcometoGadi-QueueStatus).   On job completion, contents of the job\u2019s standard output/error stream gets copied to a file in the working directory with the respective format: `.o` and `.e`. Users should check these two log files before proceeding with post-processing of any output from their corresponding job."},{"location":"model_evaluation/","title":"Model Evaluation and Diagnostics (MED)","text":""},{"location":"model_evaluation/#what-is-med-about","title":"What is MED about?","text":"<p>ACCESS-NRI's \"Model Evaluation and Diagnostics\" work is a critical facet of climate modeling, encompassing various tasks designed to ensure the model's reliability and accuracy.</p> <p>Evaluation involves scrutinizing the model through Model/Observation confrontations, checking its performance against real-world observations. It also includes experiment comparisons, testing the model under different scenarios, and inter-model comparisons like the Coupled Model Intercomparison Project (CMIP), assessing how the ACCESS-NRI model fares when compared with other climate models.</p> <p>Diagnostics involves constant monitoring of model runs to detect any anomalies or inconsistencies and a thorough analysis of outputs to verify the model's accuracy over time.</p>"},{"location":"model_evaluation/#access-med-data-and-tools-hosted-on-gadi","title":"ACCESS-MED data and tools hosted on Gadi","text":"<p>If you are new to model evaluation and diagnostics, we recommend you read our Getting Started with MED page. Here, we provide catalogs and pointers to observational data as well as model data that can be used for evaluation. We also provide a number of frameworks for model evaluation. We are also working on implementing more frameworks and recipes as well as formatting tools for a better model evaluation and diagnostics.</p>"},{"location":"model_evaluation/#getting-started","title":"Getting Started","text":"Computing Access MED Conda Environment Model Variables"},{"location":"model_evaluation/#data-catalogs","title":"Data Catalogs","text":"Observational Data Catalog Model Data Catalog"},{"location":"model_evaluation/#supported-community-frameworks-on-gadi","title":"Supported Community Frameworks on Gadi","text":"ILAMB ESMValTool COSIMA cookbook"},{"location":"model_evaluation/#tools-in-development","title":"Tools in development","text":"<p>We are currently setting up a range of tools that will help you to better evaluate and diagnose climate models: * Model Diagnostics for on-the-fly analysis of your models at different snapshots * Data format processing tools like APP4 * An Evaluation Recipe Gallery with searching functionality</p> <p>While we are working on these, we have collected a number of links to existing tools in our community tab (note that we are not currating them).</p> The documentation on Hive is work in progress! <p>The ACCESS-Hive is a community resource that is a work in progress. We\u2019d love to receive your contribution. Please see the contributing guidelines below for how to make contributions to the Hive page content. You can also open an issue highlighting any content you\u2019d like us to provide but aren\u2019t able to contribute yourself.</p>"},{"location":"model_evaluation/model_evaluation_data_processing/","title":"Data Processing Tools","text":"<p>On this page, we will provide you a list of currated data processing tools.</p> <p>While we are still ramping up this service, please take a look at the gallery of community tools on Community Resources -&gt; Community Data Processing Tools .</p>"},{"location":"model_evaluation/model_evaluation_live_diagnostics/","title":"Live Diagnostics on Gadi","text":"<p>Here, we will describe the tools we provide for \"live-diagnostics\" of the ACCESS configurations. These tools allow you to check model output/progress/failures at specified time steps while your model is running.</p>"},{"location":"model_evaluation/model_evaluation_observational_catalogs/","title":"Observational Data Catalog","text":"<p>We are working in cooperation with NCI to currate observational data collections for you that are being hosted by NCI. ACCESS-NRI and NCI are actively currating the data collections and we are working on including new data collections.</p> <p>You can search the available data collections on the NCI Data Collections website:</p> NCI Data Collections ESGF Data at NCI <p>In particular, we want to highlight the Coupled Model Intercomparison Project Phases 6 and 5 that are hosted by NCI as a sponsor of the Earth System Grid Federation (ESGF). The ESGF are federated data centres across the globe that enable access to the largest archive of climate data world-wide. This portal allows you to find, select and download data files from the federation.</p>"},{"location":"model_evaluation/model_evaluation_recipe_gallery/","title":"Model Evaluation and Diagnostics (MED) Recipe Gallery","text":"<p>While we are still building this gallery, please have a look at the Community MED Recipes listed at Community Resources -&gt; Community Model Evaluation Recipes {{ community }}</p> <p>Here, we plan to provide you with an embedded link to our actively maintained Model Evaluation and Dianostics (MED) Recipe Gallery, hosted at medportal.herokuapp.com. For now, we provide a placeholder image with link and pointers to useful Model Evaluation and Dianostics (MED) resources.</p>"},{"location":"model_evaluation/model_evaluation_recipe_gallery/#link-to-our-med-recipe-gallery","title":"Link to our MED Recipe Gallery","text":""},{"location":"model_evaluation/model_evaluation_getting_started/","title":"Getting Started with Model Evaluation at NCI","text":"<p>Welcome to Model Evaluation and Diagnostics!</p> <p>Here, we provide you the important information to give you access to the large data that we curate at NCI's storage and show you how you can use it to figure out how fit for purpose specific models are, in particular when you compare them to osbervational data:</p> Computing Access MED Conda Environment Model Variables"},{"location":"model_evaluation/model_evaluation_getting_started/access_to_gadi_at_nci/","title":"Getting Started: Computing Access (Gadi@NCI)","text":"<p>Here, we provide you the important information to give you access to the large data that we curate at NCI's storage:</p> <p>1) Get an NCI Account 2) Join relevant NCI projects 3) Logging in to Gadi@NCI 4) Computing on Gadi</p>"},{"location":"model_evaluation/model_evaluation_getting_started/access_to_gadi_at_nci/#1-nci-account","title":"1) NCI Account","text":"<p>To be able to work with our data, you need an NCI account.</p> <p>If you don't have one yet, signup here.</p> <p>Note: You will need an institutional email address with an organisation that allows access to NCI (e.g., CSIRO, a university, etc.).</p> <p>Once you have signed up, you will be allocated a username. We will refer to this username (e.g. <code>kf1234</code>) as <code>$USER</code>.</p>"},{"location":"model_evaluation/model_evaluation_getting_started/access_to_gadi_at_nci/#2-join-relevant-nci-projects","title":"2) Join relevant NCI projects","text":"<p>There is a plethora of NCI projects that may or may not be relevant for you.</p> <p>We recommend you have a chat with your supervisor to identify the relevant projects, but in any case suggest to join <code>xp65</code> for MED code as well as <code>kj13</code> for MED data.</p> <p>To get this conversation started, we list some possibly relevant projects below:</p> Project Description with link, * indicated compute resource ACCESS-NRI projects tm70 ACCESS-NRI Working Project * iq82 ACCESS-NRI MED Compute * kj13 ACCESS-NRI MED Data Dev ct11 ACCESS-NRI Replicated Datasets xp65 ACCESS-NRI Analysis Environments ACCESS projects access ACCESS software sharing p66 ACCESS - AOGCM / suppport development of the ACCESS modelling system * p73 ACCESS Model Output Archive (AOGCM) Data projects hh5 Climate-LIEF Data Storage ub7 Seasonal Prediction ACCESS-S1 Hindcast ux62 Seasonal Prediction ACCESS-S2 Hindcast cb20 ESGF CMIP3 Replication Data al33 ESGF CMIP5 Replication Data rr3 ESGF CMIP5 Australian Data Publication oi10 ESGF CMIP6 Replication Data fs38 ESGF CMIP6 Australian Data Publication rt52 ERA5 Replicated Data: Single and pressure-levels data uc16 ERA5 Replicated Datasets on Potential Temperature &amp; Vorticity Levels zz93 ERA5-Land Replicated Data zv2 Australian Gridded Climate Data (AGCD) Collection qv56 Reference Datasets for Climate Model Analysis/Forcing cj50 COSIMA Model Output Collection Other projects ik11 COSIMA shared working space v45 Ocean Extremes * ga6 Modelling the formation of sedimentary basins and continental margins * m18 Evolution and dynamics of the Australian lithosphere * q97 Earth dynamics and resources over the last billion years * qu79 Collaborative REAnalysis Technical Environment Intercomparison Project (CREATE-IP) <p>To join a project or find more projects, please use this NCI website.</p> <p>The first project that you join will become your default login project, e.g. <code>xp65</code>. We will refer to it as <code>$PROJECT</code> and we show you how to change it below.</p>"},{"location":"model_evaluation/model_evaluation_getting_started/access_to_gadi_at_nci/#3-logging-in-to-gadinci","title":"3) Logging in to Gadi@NCI","text":"<p>If you have never logged onto Gadi before, we recommend to take a look at NCI's Welcome to Gadi website. It provides all the important commands and information for logging properly onto Gadi, like the following: \"To run jobs on Gadi, you need to first log in to the system. Users on Mac/Linux can use the built-in terminal. For Windows users, we recommend using MobaXterm as the local terminal. Logging in to Gadi happens through a Gadi login node.\"</p> <p>When you login, via the command <pre><code>ssh -Y $USER@gadi.nci.org.au\n</code></pre> you will enter your $HOME directory with your default <code>$PROJECT</code> and your default SHELL. Both are saved at <code>$HOME/.config/gadi-login.conf</code> and you can print them via <pre><code>cat $HOME/.config/gadi-login.conf\n</code></pre></p> <p>The <code>-Y</code> option is needed to run graphical tools by enabling the forwarding of trusted X protocol mesgs between X-Server on local system and X programs on Gadi.  You need to enable X Windowing system on your local system before running ssh. This can be done by running X-Server like XQuartz (Mac), MobaXterm (MS Windows), startx or similar (Linux).</p> <p>Again, for more useful information we recommend to check out NCI's Welcome to Gadi website.</p>"},{"location":"model_evaluation/model_evaluation_getting_started/access_to_gadi_at_nci/#4-computing-on-gadi","title":"4) Computing on Gadi","text":""},{"location":"model_evaluation/model_evaluation_getting_started/access_to_gadi_at_nci/#gadi-resources","title":"Gadi Resources","text":"<p>Coupled climate models like ACCESS-CM involve, among other things, calculation of complex mathematical equations that explain the physics of the atmosphere and oceans. Performed at hundreds of millions of points around the Earth, these calculations require vast computing power to complete them in a reasonable amount of time, thus relying on the power of  high-performance computing (HPC) like Gadi. The Gadi supercomputer can handle more than 10 million billion (10 quadrillion) calculations per second and is connected to 100,000 Terabytes of high-performance research data storage.</p> <p>An overview of Gadi resources such as compute, storage and PBS jobs are described below. </p> <p>Useful NCI commands to check your available compute resources are:</p> Command Purpose <code>logout</code> or Ctrl+D To exit a session <code>hostname</code> Displays login node details <code>module list</code> Modules currently loaded <code>module avail</code> Available modules <code>nci_account -P [proj]</code> Compute allocation for [proj] <code>nqstat -P [proj]</code> Jobs running/queued in [proj] <code>lquota</code> Storage allocation and usage for all your projects"},{"location":"model_evaluation/model_evaluation_getting_started/access_to_gadi_at_nci/#compute-hours","title":"Compute Hours","text":"<p>Compute allocations are granted to projects instead of directly to users and, hence, you need to be a member of a project in order to use its compute allocation. To run jobs on Gadi, you need to have sufficient allocated compute hours available, where the job cost depends on the resources reserved for the job and the amount of walltime it uses. </p>"},{"location":"model_evaluation/model_evaluation_getting_started/access_to_gadi_at_nci/#storage","title":"Storage","text":"<p>Each user has a project-independent <code>$HOME</code> directory, which has a storage limit of 10 GiB. All data on <code>/home</code> is backed up.</p> <p>Through project membership, the user gets access to the storage space within the project folders <code>/scratch</code> and  <code>/g/data</code> filesystems for that particular project.</p>"},{"location":"model_evaluation/model_evaluation_getting_started/access_to_gadi_at_nci/#pbs-jobs","title":"PBS Jobs","text":"<p>To run compute tasks such as an ACCESS-CM suite on Gadi, users need to submit them as jobs to queues. Within a job submission, you can specify the queue, duration and computational resources needed for your job. When a job submission is accepted, it is assigned a jobID (shown in the return message) that can then be used to monitor the job\u2019s status. </p> <p>On job completion, contents of the job\u2019s standard output/error stream gets copied to a file in the working directory with the respective format: <code>&lt;jobname&gt;.o&lt;jobid&gt;</code> and <code>&lt;jobname&gt;.e&lt;jobid&gt;</code>. Users should check these two log files before proceeding with post-processing of any output from their corresponding job.</p>"},{"location":"model_evaluation/model_evaluation_getting_started/model_evaluation_getting_started/","title":"Model Evaluation Environment at Gadi@NCI","text":"<p>At this stage of Getting Started, we assume that you already have access to Gadi@NCI. If this is not the case, please go to our instructions on how to get access to Gadi@NCI.</p> <p>Here we describe where you can find, load, and evalulate observational and model data on Gadi.</p> ACCESS-NRI provides code and data, but not computing resources <p>You do not automatically have access to all of Gadi's storage at <code>/g/data/</code>, but need to be part of a <code>$PROJECT</code> to see files at <code>/g/data/$PROJECT</code>. Furthermore, if you use Gadi's job submission system PBS (Portable Batch System), you need to add the relevant storage to the <code>#PBS -l storage=gdata/xp65+gdata/kj13</code> (if you want the job to have access to <code>xp65</code> and <code>kj13</code> in this example).</p>"},{"location":"model_evaluation/model_evaluation_getting_started/model_evaluation_getting_started/#1-access-med-our-currated-conda-environment-on-gadi","title":"1 <code>access-med</code>: Our currated <code>conda</code> environment on Gadi","text":"<p>To avoid running multiple (different) versions of code on Gadi, we provide you with a <code>conda</code> environment called <code>access-med</code> that we actually curate for you (version 0.1 is from June 2023).</p> <p>In order to change to this environment, please execute the following commands after loggin onto Gadi (and as part of your PBS scripts): <pre><code>$ module use /g/data/xp65/public/modules\n$ module load conda/access-med\n</code></pre></p> <p>If you are planning to run your code through JupyterLab on NCI's ARE, you need to use <code>/g/data/xp65/public/modules</code> as Module directories and <code>conda/are</code> as Modules when launching a JupyterLab session.</p>"},{"location":"model_evaluation/model_evaluation_getting_started/model_evaluation_getting_started/#2-what-is-part-of-the-access-med-enrivonment","title":"2 What is part of the <code>access-med</code> enrivonment?","text":"<p>You are now able to use the scripts of our currated environment, including <code>python3</code>, <code>intake</code>, <code>jupyter</code>, <code>esmvaltool</code>, or <code>ilamb</code>. The complete list of dependencies can be found in the <code>environment.yml</code> file of our GitHub repository:</p> <p></p>"},{"location":"model_evaluation/model_evaluation_getting_started/model_variables/","title":"Model Output and Variables","text":"<p>Speaking a common language and using the same variables is key for a united climate modelling community. While we are trying to bring together modellers from different communities, we understand that the variables used for climate modelling may differ for historical reasons. Here, we are collating lists of different widely used variable formats.</p>"},{"location":"model_evaluation/model_evaluation_getting_started/model_variables/#variables-for-cmip-version-6coupled-model-intercomparison-project","title":"Variables for CMIP version 6(Coupled Model Intercomparison Project)","text":"<p>You can search the extensive list of Coupled Model Intercomparison Project version 6 on this website.</p>"},{"location":"model_evaluation/model_evaluation_getting_started/model_variables/#variables-for-era5-atmospheric-reanalysis","title":"Variables for ERA5 atmospheric reanalysis","text":"<p>ERA5 is the fifth generation ECMWF atmospheric reanalysis of the global climate covering the period from January 1940 to present. ERA5 is produced by the Copernicus Climate Change Service (C3S) at ECMWF. ERA5 provides hourly estimates of a large number of atmospheric, land and oceanic climate variables.</p> <p>A full list of ERA5 parameters is available on the ECMWF database. It covers both the ERA5 parameter listings as well as the ERA5-LAND parameter listings.</p>"},{"location":"model_evaluation/model_evaluation_model_catalogs/","title":"ACCESS-NRI intake Model Catalog","text":"<p>ACCESS-NRI is hosting a number of calculated models for you through National Computational Infrastructure (NCI) storage.</p> <p>We have set up an ACCESS-NRI intake Catalog package that allows you to easily search and load the model data on this storage. The premise of this ACCESS-NRI intake Catalog is to provide a (\"meta\") catalog of intake-esm (\"sub\") catalogs, which each correspond to different \"experiments\".</p>"},{"location":"model_evaluation/model_evaluation_model_catalogs/#the-access-nri-intake-catalog","title":"The ACCESS-NRI intake catalog","text":"<p>To have the huge amount of data from different experiments on the NCI storage at the palm of your hand, we provide a (\"meta\") catalog for you to query via python as part of the <code>intake</code> package with our curated catalog plugin <code>intake.cat.access_nri</code> .</p> <pre><code>import intake\naccess_nri_catalog_sections = intake.cat.access_nri\n</code></pre> <p>To use this catalog, you need access to NCI's Gadi. Check out our Get Started with ACCESS at NCI   guide on how to get access.</p> <p>Once logged in to Gadi, you will need to add the <code>access-nri-catalog</code> to your <code>conda</code> environments and start an ARE JupyterLab Session. Check out our ACCESS-NRI Intake Catalog guide  for the specific setup (note that you can only read in data from specific experiments if they are loaded through the Storage keyword).</p> <p>Once your JupyterLab session started, you can access the <code>intake</code> catalog to load the data. Take a look at this Tutorial .</p>"},{"location":"model_evaluation/model_evaluation_model_catalogs/#example-search-with-our-intake-catalog","title":"Example Search with our intake catalog","text":"<pre><code># Impport packages for searching/loading/plotting\nimport intake\nfrom distributed import Client\nimport matplotlib.pyplot as plt\n\n# The search process is a 2-step one\n# Comparable with searching for a book in a library:\n# 1) You look for the right book/catalog sections\n# 2) You look for the right book/catalog in the these sections\n\n# Load the ACCES-NRI list of catalogs for available experiment data\n# Similar to an overview of library section\naccess_nri_catalog_sections = intake.cat.access_nri\n\n# Perform a search for names, models, variables etc.\nexample_section_search = access_nri_catalog_sections.search(name=\"cmip6_oi10\")\n\n# Once you are sufficiently happy with your search, you can load the \"section\"\ncatalog_sections = access_nri_catalog_sections.search(name=\"025deg_jra55_iaf_omip2_cycle1\").to_source()\n# and start looking for the right catalogs of interest\ncatalogs_of_interest = catalog_sections.search(filename=\"ocean_scalar.*\")\n\n# Call the client that allows use load the data efficiently\nclient = Client(threads_per_worker=1)\nclient.dashboard_link\n\n# Actually load the data\nexperiment_data = catalogs_of_interest.to_dataset_dict(progressbar=False)\n\n# Et voil\u00e0, you have loaded the data and can start plotting\nexperiment_data[\"ocean_scalar_snapshot.1day\"][\"temp_global_ave\"].plot(label=\"daily\")\nexperiment_data[\"ocean_scalar.1mon\"][\"temp_global_ave\"].plot(label=\"monthly\")\n_ = plt.legend()\n</code></pre>"},{"location":"model_evaluation/model_evaluation_model_catalogs/model_evaluation_add_models/","title":"Add your model data to the ACCESS-NRI intake Catalog","text":"<p>You've just run a new experiment, now you want to create an intake-esm catalog for that experiment?</p> <p>Look at this Tutorial  to learn how to add your own models.</p>"},{"location":"model_evaluation/model_evaluation_model_catalogs/model_evaluation_search_models/","title":"Search for a model in the ACCESS-NRI intake Catalog","text":"<p>To have the huge amount of data from different experiments on the NCI storage at the palm of your hand, we provide a (\"meta\") catalog for you to query via python as part of the <code>intake</code> package with our curated catalog plugin <code>intake.cat.access_nri</code> .</p> <p>To use this catalog, you need access to NCI's Gadi. Check out our Get Started with ACCESS at NCI   guide on how to get access.</p> <p>Once logged in to Gadi, you will need to add the <code>access-nri-catalog</code> to your <code>conda</code> environments and start an ARE JupyterLab Session. Check out our ACCESS-NRI Intake Catalog guide  for the specific setup (note that you can only read in data from specific experiments if they are loaded through the Storage keyword).</p> <p>Once your JupyterLab session started, you can access the <code>intake</code> catalog to load the data. Take a look at this Tutorial .</p> <pre><code># Impport packages for searching/loading/plotting\nimport intake\nfrom distributed import Client\nimport matplotlib.pyplot as plt\n\n# The search process is a 2-step one\n# Comparable with searching for a book in a library:\n# 1) You look for the right book/catalog sections\n# 2) You look for the right book/catalog in the these sections\n\n# Load the ACCES-NRI list of catalogs for available experiment data\n# Similar to an overview of library section\naccess_nri_catalog_sections = intake.cat.access_nri\n\n# Perform a search for names, models, variables etc.\nexample_section_search = access_nri_catalog_sections.search(name=\"cmip6_oi10\")\n\n# Once you are sufficiently happy with your search, you can load the \"section\"\ncatalog_sections = access_nri_catalog_sections.search(name=\"025deg_jra55_iaf_omip2_cycle1\").to_source()\n# and start looking for the right catalogs of interest\ncatalogs_of_interest = catalog_sections.search(filename=\"ocean_scalar.*\")\n\n# Call the client that allows use load the data efficiently\nclient = Client(threads_per_worker=1)\nclient.dashboard_link\n\n# Actually load the data\nexperiment_data = catalogs_of_interest.to_dataset_dict(progressbar=False)\n\n# Et voil\u00e0, you have loaded the data and can start plotting\nexperiment_data[\"ocean_scalar_snapshot.1day\"][\"temp_global_ave\"].plot(label=\"daily\")\nexperiment_data[\"ocean_scalar.1mon\"][\"temp_global_ave\"].plot(label=\"monthly\")\n_ = plt.legend()\n</code></pre>"},{"location":"model_evaluation/model_evaluation_on_gadi/","title":"Model Evaluation on Gadi/NCI","text":"<p>We are providing support for an increasing amount of frameworks and recipes on Gadi/NCI.</p> <p>At the moment, we are actively supporting:</p> ILAMB ESMValTool COSIMA cookbook <p>The best way to get our help is by raising an issue on the community forum with tags <code>help</code> and another tag for the specific framework.</p> <p>In the future, we are also aiming to support a broader range of frameworks and recipes.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/","title":"Tutorial for using <code>esmvaltool</code> on Gadi@NCI","text":"The documentation on Hive is work in progress! <p>The ACCESS-Hive is a community resource that is a work in progress. We\u2019d love to receive your contribution. Please see the contributing guidelines below for how to make contributions to the Hive page content. You can also open an issue highlighting any content you\u2019d like us to provide but aren\u2019t able to contribute yourself.</p> <p>ACCESS ESMValTool Worflow recipe status</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/#about-esmvaltool","title":"About ESMValTool","text":"<p>The Earth System Model Evaluation Tool (ESMValTool) is a community-development that aims at improving diagnosing and understanding of the causes and effects of model biases and inter-model spread. The ESMValTool mainly focus on evaluating results from the Coupled Model Intercomparison Project (CMIP) ensemble. The goal is to build a common framework for the evaluation of Earth System Models (ESMs) against observations available through the Earth System Grid Federation (ESGF) in standard formats (obs4MIPs) or made available at ESGF nodes.</p> <p>More information on ESMValTool scope is available in the extensive ESMValTool documentation.</p> <p>ACCESS-NRI provides access to the latest version of ESMValTool via the xp65 access-med conda environment deployed on NCI-Gadi. Our plan is to routinely run benchmarks and comparisons of the ACCESS models CMIP submissions. We will also provide support for running recipes on NCI-Gadi.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/#running-esmvaltool-on-gadi","title":"Running <code>esmvaltool</code> on Gadi","text":""},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/#pre-requisites","title":"Pre-requisites","text":"<p>NCI Projects requires to run the set of ESMValTool recipes:</p> <ul> <li>xp65, kj13, fs38, oi10, rr3, al33, rt52, zz93, qv56</li> </ul>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/#load-the-access-med-conda-environment","title":"Load the <code>access-med</code> conda environment","text":"<pre><code>    module use /g/data/xp65/public/modules\n    module load conda/access-med\n</code></pre>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/#what-recipes-are-available","title":"What recipes are available?","text":"<pre><code>    esmvaltool recipes list\n</code></pre>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/#details-of-a-recipe","title":"Details of a recipe","text":"<pre><code>esmvaltool recipes show recipe_name.yml\n</code></pre>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/#running-an-recipe-yourself","title":"Running an recipe yourself","text":"<pre><code>esmvaltool run examples/recipe_python.yml --search_esgf=when_missing\n</code></pre>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/#support","title":"Support","text":"<p>ACCESS and NCI-Gadi users can get help from ACCESS-NRI for running their recipe on Gadi via Github Issue on the ESMValTool-Workflow github repository or by opening a thread on the ACCESS-Hive Forum.</p> <p>General support for ESMValTool (non-specific to NCI-Gadi) can be found in ESMValTool Discussions page where users can open an issue and a member of the User Engagement Team of ESMValTool will reply as soon as possible. This is open for all general and technical questions on the ESMValTool: installation, application, development, or any other question or comment you may have.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/#recipes-and-diagnostics","title":"Recipes and diagnostics","text":"<p>Contacts for specific diagnostic sets are the respective authors, as listed in the corresponding recipe and diagnostic documentation and in the source code.</p> <p>The current status of ESMValTool recipes for the xp64 conda environment is available here</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/#license","title":"License","text":"<p>The ESMValTool is released under the Apache License, version 2.0. Citation of the ESMValTool paper (\u201cSoftware Documentation Paper\u201d) is kindly requested upon use, alongside with the software DOI for ESMValTool (doi:10.5281/zenodo.3401363) and ESMValCore (doi:10.5281/zenodo.3387139) and version number:</p> <p>Righi, M., Andela, B., Eyring, V., Lauer, A., Predoi, V., Schlund, M., Vegas-Regidor, J., Bock, L., Br\u00f6tz, B., de Mora, L., Diblen, F., Dreyer, L., Drost, N., Earnshaw, P., Hassler, B., Koldunov, N., Little, B., Loosveldt Tomas, S., and Zimmermann, K.: Earth System Model Evaluation Tool (ESMValTool) v2.0 \u2013 technical overview, Geosci. Model Dev., 13, 1179\u20131199, https://doi.org/10.5194/gmd-13-1179-2020, 2020.</p> <p>Besides the above citation, users are kindly asked to register any journal articles (or other scientific documents) that use the software at the ESMValTool webpage (http://www.esmvaltool.org/). Citing the Software Documentation Paper and registering your paper(s) will serve to document the scientific impact of the Software, which is of vital importance for securing future funding. You should consider this an obligation if you have taken advantage of the ESMValTool, which represents the end product of considerable effort by the development team.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_esmvaltool/#esmvaltool-recipes-examples","title":"ESMValTool recipes examples","text":"<p>Below you can find the recipes from <code>esmvaltool</code> that we are providing to run on Gadi. The original recipes are </p>        recipe_flato13ipcc.yml        recipe_perfmetrics_CMIP5.yml        recipe_ecs_scatter.yml        recipe_flato13ipcc.yml        recipe_flato13ipcc.yml        recipe_diurnal_index.yml        recipe_crem.yml        recipe_collins13ipcc.yml        recipe_autoassess_stratosphere.yml        recipe_lauer13jclim.yml        recipe_zmnam.yml        recipe_russell18jgr.yml        recipe_sea_surface_salinity.yml        recipe_schlund20esd.yml        recipe_runoff_et.yml        recipe_perfmetrics_CMIP5.yml        recipe_spei.yml        recipe_runoff_et.yml        recipe_hyint.yml        recipe_cox18_nature.yml        recipe_extreme_events.yml        recipe_smpi.yml        recipe_landcover.yml        recipe_miles_block.yml        recipe_esacci_oc.yml        recipe_ocean_ice_extent.yml        recipe_ocean_amoc.yml"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/","title":"Tutorial for using <code>ilamb</code> on Gadi@NCI","text":"<p>This tutorial explains how you can setup and run International Land Model Benchmarking (ILAMB) and International Ocean Model Benchmarking (IOMB) tests on <code>NCI</code> infrastracture. Both projects are maintained as <code>python</code> code under the package name <code>ilamb</code>.</p> <p>The Tutorial contains:  </p> <ol> <li>Background </li> <li>Installation guide </li> <li>Setup details </li> <li>Run <code>ilamb</code> </li> <li>Run <code>liamb</code> on <code>NCI</code> </li> <li>Fix your setup with <code>ilamb-doctor</code> </li> </ol>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#1-background-international-land-model-benchmarking-ilamb-and-international-ocean-model-benchmarking-iomb","title":"1. Background: International Land Model Benchmarking (ILAMB) and International Ocean Model Benchmarking (IOMB)","text":"<p>As earth system models (ESMs) become increasingly complex, there is a growing need for comprehensive and multi-faceted evaluation of model projections. The International Land Model Benchmarking (ILAMB) project is a model-data intercomparison and integration project designed to improve the performance of land models and, in parallel, improve the design of new measurement campaigns to reduce uncertainties associated with key land surface processes.</p> <p>If you have used (and installed) <code>ilamb</code> on NCI and know the basic principle of <code>ilamb</code>, you can start from Section 5) Guide for using ilamb on NCI.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#2-installing-ilamb","title":"2. Installing <code>ilamb</code>","text":"<p>For NCI users, ACCESS-NRI is providing a <code>conda</code> environment called <code>ilamb_dev</code> through the <code>xp65</code> project, with ilamb installed. You can load and activate it via:</p> <pre><code>&gt;&gt;&gt; module use /g/data/xp65/public/modules\n&gt;&gt;&gt; module load conda/ilamb_dev\n&gt;&gt;&gt; conda activate ilamb_dev\n</code></pre> <p>We will soon add <code>ilamb</code> also to the ACCESS-NRI MED <code>conda</code> environment, <code>access-med</code> under project<code>xp65</code>.</p> <p>If you want to install <code>ilamb</code> yourself, please follow the official installation instructions at https://www.ilamb.org/doc/install.html.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#3-configuring-ilamb","title":"3. Configuring <code>ilamb</code>","text":"<p>Before you can run <code>ilamb</code>, you need to configure a few things:</p> <p>3.1. Organise the ILAMB_ROOT path 3.2. Set up a <code>config</code> file 3.3. Set up a <code>modelroute</code> and <code>regions</code> files (Optional, if you want to run only a subset of models and/or specific regions of the world) 3.4. Download a <code>shapefiles</code> files locally (Optional online, necessary offline e.g. on NCI compute nodes)</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#31-organise-the-ilamb_root-path","title":"3.1 Organise the ILAMB_ROOT path","text":"<p><code>ilamb</code> demands files to be organised in a specific directory structure of <code>DATA</code> and <code>MODELS</code>.</p> <p>If you do not have your own files yet, you can download and use example files provided as part of the  of <code>ilamb</code>'s First Steps Tutorial</p> <p>The following tree represents the organization of the contents of this extracted sample data (Note: We renamed the main directory name): <pre><code>$ILAMB_ROOT (renamed from \"ILAMB_sample\")\n\u251c\u2500\u2500 sample.cfg (see [Section 3.2](#32-set-up-a-config-file))\n\u251c\u2500\u2500 modelroute.txt (optional, see [Section 3.3](#33-set-up-modelroute-and-regions-files))\n\u251c\u2500\u2500 regions.txt (optional, see [Section 3.3](#33-set-up-modelroute-and-regions-files))\n\u251c\u2500\u2500 DATA\n\u2502   \u251c\u2500\u2500 albedo\n\u2502   \u2502   \u2514\u2500\u2500 CERES\n\u2502   \u2502       \u2514\u2500\u2500 albedo_0.5x0.5.nc\n\u2502   \u2514\u2500\u2500 rsus\n\u2502       \u2514\u2500\u2500 CERES\n\u2502           \u2514\u2500\u2500 rsus_0.5x0.5.nc\n\u2514\u2500\u2500 MODELS\n    \u2514\u2500\u2500 CLM40cn\n        \u251c\u2500\u2500 rsds\n        \u2502   \u2514\u2500\u2500 rsds_Amon_CLM40cn_historical_r1i1p1_185001-201012.nc\n        \u2514\u2500\u2500 rsus\n            \u2514\u2500\u2500 rsus_Amon_CLM40cn_historical_r1i1p1_185001-201012.nc\n</code></pre></p> <p>There are two main branches in this directory. The first is the <code>DATA</code> directory\u2013this is where we keep the observational datasets each in a subdirectory bearing the name of the variable. While not strictly necesary to follow this form, it is a convenient convention. The second branch is the <code>MODEL</code> directory in which we see a single model result from CLM.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#311-add-files-to-data","title":"3.1.1 Add files to DATA","text":"<p>There is a lot of DATA available to add. Take a look at https://www.ilamb.org/ILAMB-Data/ and https://www.ilamb.org/IOMB-Data/ for extensive lists for ILAMB-Data (land modelling) and IOMB-Data (ocean modelling).</p> <p><code>ilamb</code> has a commandline prompt to add new DATA in a substructure. To fetch all available DATA from the website, you can simply go to your $ILAMB_ROOT and type <pre><code>&gt;&gt;&gt; ilamb-fetch\n</code></pre></p> <p>The command will compare the above website with your current DATA and make suggestions for downloads: <pre><code>Comparing remote location:\n\n      https://www.ilamb.org/ILAMB-Data/\n\nTo local location:\n\n      ./\n\nI found the following files which are missing, out of date, or corrupt:\n\n      .//DATA/twsa/GRACE/twsa_0.5x0.5.nc\n      .//DATA/rlus/CERES/rlus_0.5x0.5.nc\n      ... (we have abbreviated the extensive list here)\n\nDownload replacements? [y/n]\n</code></pre></p> <p>You can use <code>ilamb-fetch -h</code> to see how you can adjust the remote and local locations. If you want to download IOMB-Data, you can for example use <pre><code>ilamb-fetch --remote_root https://www.ilamb.org/IOMB-Data/\n</code></pre></p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#312-add-files-to-model","title":"3.1.2 Add files to MODEL","text":"<p>If you want to add your own <code>MODEL</code> to <code>ilamb</code>, you can do so by following this description.</p> <p>For <code>NCI</code> users, our <code>ilamb_dev</code> <code>conda</code> enrivonment already provides all observational datasets from the <code>ilamb</code> official web and the ACCESS-ESM1_5 model result for user at <code>ILAMB_ROOT</code>. Stay tune for more observational and model data or tell us which ones we should definitely add.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#32-set-up-a-config-file","title":"3.2 Set up a <code>config</code> file","text":"<p>Now that we have the data, we need to setup a <code>config</code> file which the <code>ilamb</code> package will use to initiate a benchmark study.  </p> <p><code>ilamb</code> provides default config files here.</p> <p>Below we explain both which variables you can define, but start by showing you the minimum setup from the tutorial's. <code>sample.cfg</code> file:</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#minimum-configure-file-with-a-direct-and-a-derived-variable","title":"Minimum configure file with a direct and a derived variable","text":"<pre><code># This configure file specifies the variables\n\n[h1: Radiation and Energy Cycle]\n\n[h2: Surface Upward SW Radiation]\nvariable = \"rsus\"\n\n[CERES]\nsource   = \"DATA/rsus/CERES/rsus_0.5x0.5.nc\"\n\n[h2: Albedo]\nvariable = \"albedo\"\nderived  = \"rsus/rsds\"\n\n[CERES]\nsource   = \"DATA/albedo/CERES/albedo_0.5x0.5.nc\"\n</code></pre> <p>In brief: This file allows you to create different header descriptions of the experiments (<code>h1</code>: top level for grouping of variables, <code>h2</code>: sub-level for each variable), but most importantly the <code>variable</code>s that we will look into and their <code>source</code>. In the eaxmple, <code>rsus</code> (Surface Upward Shortwave Radiation) and <code>albedo</code> are the used variables. The latter is actually derived from two variables by dividing the Surface Upward Shortwave Radiation by the Surface Downward Shortwave Radiation, <code>derived = rsus/rsds</code>. Finally, sources are defined as <code>source</code> with a text-font header without <code>h1</code> or <code>h2</code>.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#changing-configure-file-variables","title":"Changing configure file variables","text":"<p>This is the list of all the variables you can modify in config file: <pre><code>source              = None\n#Full path to the observational dataset\n\ncmap                = \"jet\"\n#The colormap to use in rendering plots (default is 'jet')\n\nvariable            = None\n#Name of the variable to extract from the source dataset\n\nalternate_vars      = None\n#Other accepted variable names when extracting from models\n\nderived             = None\n#An algebraic expression which captures how the confrontation variable may be generated\n\nland                = False\n#Enable to force the masking of areas with no land (default is False)\n\nbgcolor             = \"#EDEDED\"\n#Background color\n\ntable_unit          = None\n#The unit to use when displaying output in tables on the HTML page\n\nplot_unit           = None\n#The unit to use when displaying output on plots on the HTML page\n\nspace_mean          = True\n#Disable to compute sums of the variable over space instead of mean values\n\nrelationships       = None\n#A list of confrontations with whose data we use to study relationships, the syntax is \"h2 tag/observational dataset\". You will see the relationship part in the output if you specify some relationship.\n\nctype               = None\n#Choose a specific Confrontion class. \n\nregions             = None\n#Specify the regions of confrontation\n\nskip_rmse           = False\n#akip rmse in program\n\nskip_iav            = True\n#Ship iav in program\n\nmass_weighting      = False\n#if switch to true, using an average data in a period to normalize\n\nweight              = 1    \n# if a dataset has no weight specified, it is implicitly 1\n</code></pre></p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#33-set-up-modelroute-and-regions-files","title":"3.3 Set up <code>modelroute</code> and <code>regions</code> files","text":"<p>If you plan to run only a specific subset of models, you can already define them in a <code>modelroute.txt</code> file. It could look like our specific example for running different versions (1, 2, and 3) of the ACCESS-ESM 1.5 suite:</p> <pre><code># Model Name                    , PATH/TO/MODELS  , EXTRA COMMANDS\nACCESS_ESM1-5-r1i1p1f1          , MODELS/r1i1p1f1 , CMIP6\nACCESS_ESM1-5-r2i1p1f1          , MODELS/r2i1p1f1 , CMIP6\nACCESS_ESM1-5-r3i1p1f1          , MODELS/r3i1p1f1 , CMIP6\n... (abbreviated)\n</code></pre>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#34-downloadlink-shapefiles-files-locally","title":"3.4 Download/link <code>shapefiles</code> files locally","text":"<p>You can download the <code>shapefiles</code> that are needed to run <code>ilamb</code> and <code>cartopy</code> offline here:</p> <ul> <li>For Land: https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/physical/ne_110m_land.zip</li> <li>For Ocean: https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/physical/ne_110m_ocean.zip</li> </ul> <p>Finally, you need to define that path as <code>CARTOPY_DATA_DIR</code> via  <pre><code>export CARTOPY_DATA_DIR=/absolute/path/to/shapefiles/directory\n</code></pre></p> <p>Note: For NCI, we already provide shapefiles in a directory as part of project <code>xp65</code>. After joining the project, you can thus easily use <pre><code>export CARTOPY_DATA_DIR=/g/data/xp65/public/apps/cartopy-data\n</code></pre></p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#4-run-ilamb","title":"4. Run <code>ilamb</code>","text":""},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#41-ilamb-run","title":"4.1 ilamb-run","text":"<p>Now that we have the configuration file set up, you can run the study using the <code>ilamb-run</code> script. Executing the following command at the $ILAMB_ROOT directory will run <code>ilamb</code> as specified in your <code>sample.cfg</code> for all models of the <code>model_root</code> and all regions (<code>global</code>) of the world: <pre><code>ilamb-run --config sample.cfg --model_root $ILAMB_ROOT/MODELS/ --regions global\n</code></pre> If you are on some institutional resource, you may need to launch the above command using a submission script, or request an interactive node. As the script runs, it will yield output which resembles the following: <pre><code>Searching for model results in /Users/ncf/sandbox/ILAMB_sample/MODELS/\n\n                                          CLM40cn\n\nParsing config file sample.cfg...\n\n                   SurfaceUpwardSWRadiation/CERES Initialized\n                                     Albedo/CERES Initialized\n\nRunning model-confrontation pairs...\n\n                   SurfaceUpwardSWRadiation/CERES CLM40cn              Completed  37.3 s\n                                     Albedo/CERES CLM40cn              Completed  44.7 s\n\nFinishing post-processing which requires collectives...\n\n                   SurfaceUpwardSWRadiation/CERES CLM40cn              Completed   3.3 s\n                                     Albedo/CERES CLM40cn              Completed   3.3 s\n\nCompleted in  91.8 s\n</code></pre> What happened here? First, the script looks for model results in the directory you specified in the <code>--model_root</code> option. It will treat each subdirectory of the specified directory as a separate model result. Here since we only have one such directory, <code>CLM40cn</code>, it found that and set it up as a model in the system. Next it parsed the configure file we examined earlier. We see that it found the CERES data source for both variables as we specified it. If the source data was not found or some other problem was encountered, the green <code>Initialized</code> will appear as red text which explains what the problem was (most likely <code>MisplacedData</code>). If you encounter this error, make sure that <code>ILAMB_ROOT</code> is set correctly and that the data really is in the paths you specified in the configure file.</p> <p>Next we ran all model-confrontation pairs. In our parlance, a confrontation is a benchmark observational dataset and its accompanying analsys. We have two confrontations specified in our configure file and one model, so we have two entries here. If the analysis completed without error, you will see a green <code>Completed</code> text appear along with the runtime. Here we see that <code>albedo</code> took a few seconds longer than <code>rsus</code>, presumably because we had the additional burden of reading in two datasets and combining them.</p> <p>The next stage is the post-processing. This is done as a separate loop to exploit some parallelism. All the work in a model-confrontation pair is purely local to the pair. Yet plotting results on the same scale implies that we know the maxmimum and minimum values from all models and thus requires the communcation of this information. Here, as we are plotting only over the globe and not extra regions, the plotting occurs quickly.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#42-run-specific-models-and-regions","title":"4.2 Run specific models and regions","text":"<p>As mentioned in Section 3.3, you can adjust the models and regions that <code>ilamb</code> will run on. You can find more information in the <code>ilamb</code> tutorial. Calling <code>ilamb-run</code> with both specifications, would look like: <pre><code>ilamb-run --config cmip.cfg --model_setup modelroute.txt --regions regions.txt\n</code></pre> where you call a specific config file (see Section 3.2) as well as specific model routes and regions with files (see Section 3.3).</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#43-viewing-the-benchmarking-output-in-your-browser","title":"4.3 Viewing the benchmarking output in your browser","text":"<p>The whole process generates a directory of results within ILAMB_ROOT which by default is called <code>_build</code>. To view the results locally on your computer, navigate into this directory and start a local <code>http</code> server: <pre><code>python -m http.server\n</code></pre> You should see a message similar to this (or use http://0.0.0.0:8000/): <pre><code>Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ...\n</code></pre> Open this link in your browser and you will see a webpage with a summary table in the center. As we have so few variables and a single model at this point, the table will very simple:</p> <p></p> <p>As we add more variables and models, this summary table helps you understand relative differences in scores among models. For now, clicking on a row of the table will expand it to reveal the underlying datasets used. Clicking on CERES will take you to another page which presents detailed scores and plots.</p> <p></p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#5-run-ilamb-on-nci","title":"5. Run <code>ilamb</code> on NCI","text":"<p>If you followed the guides above, you should be familiar with how you can setup <code>ilamb</code>.</p> <p>To run <code>ilamb</code> on NCI, you can either start an interactive setup Section 5.1 or use a non-interactive Portable Batch System (PBS) job Section 5.2.</p> <p>In both cases, you need to again define the variable <code>$ILAMB_ROOT</code></p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#51-ilamb_root-and-datamodel-on-nci","title":"5.1 ILAMB_ROOT and DATA/MODEL on NCI","text":"<p>In our default setup, we will place ILAMB_ROOT and the shapefiles for cartopy directly in the home directory. First, you have to create the ILAMB_ROOT directory <pre><code>mkdir $PWD/ILAMB_ROOT\n</code></pre> You can then simply export their paths after login as: <pre><code>export ILAMB_ROOT=$PWD/ILAMB_ROOT\nexport CARTOPY_DATA_DIR=/g/data/xp65/public/apps/cartopy-data\n</code></pre></p> <p>You can of course change the path of the directory, but will need to take this into account for the PBS job by adding a command to change into the $ILAMB_ROOT directory (see PBS setup comments).</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#ilamb_rootdata-on-nci","title":"ILAMB_ROOT/DATA on NCI","text":"<p>An extensive colletion of DATA is provided in the <code>kj13</code> project. You need to have joined the project on NCI to get access to this data.</p> <p>To create a symbolic link to this data, use the bash command <pre><code>ln -s /g/data/kj13/datasets/ilamb/DATA $ILAMB_ROOT/DATA\n</code></pre></p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#ilmab_rootmodel-on-nci","title":"ILMAB_ROOT/MODEL on NCI","text":"<p>In the future, we will provide a symbolic link to a MODEL catalog for you as well.</p> <p>For now, you will need to create the directory $ILAMB_ROOT/MODEL and populate it with symbolic links to specific models yourself.</p> <p>In our example, we will use ACCESS-ESM1.5, which is provided on NCI as part of project <code>fs38</code>.  You need to have joined the project on NCI to get access to this data.</p> <p>To link the ACCESS-ESM1.5 suite files to your $ILAMB_ROOT/MODEL, simply execute the following bash command <pre><code>mkdir $ILAMB_ROOT/MODELS\nln -s /g/data/fs38/publications/CMIP6/CMIP/CSIRO/ACCESS-ESM1-5/historical/r* $ILAMB_ROOT/MODELS\n</code></pre></p> <p>By the end of Section 5.1.1 and 5.1.2, your $ILAMB_ROOT Directory should look similar to <pre><code>$ILAMB_ROOT\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 DATA -&gt; /g/data/kj13/datasets/ilamb/DATA\n\u2514\u2500\u2500 MODELS\n    \u251c\u2500\u2500 r10i1p1f1 -&gt; /g/data/fs38/publications/CMIP6/CMIP/CSIRO/ACCESS-ESM1-5/historical/r10i1p1f1\n    \u251c\u2500\u2500 ... (abbreviated)\n    \u2514\u2500\u2500 r9i1p1f1 -&gt; /g/data/fs38/publications/CMIP6/CMIP/CSIRO/ACCESS-ESM1-5/historical/r9i1p1f1\n</code></pre></p> <p>These different models have a lot of subdirectories, which are important to keep in mind when defining the <code>source</code> parameter in your <code>ilamb</code> <code>.cfg</code> file. Note that the <code>ilamb</code> files will end with <code>*.nc*. For example, one of the *rsus* files for run</code>r10i1p1f1<code>can be found (and used for</code>.cfg` under <pre><code>source = /g/data/fs38/publications/CMIP6/CMIP/CSIRO/ACCESS-ESM1-5/historical/r1i1p1f1/Amon/rsus/gn/files/d20191115/rsus_Amon_ACCESS-ESM1-5_historical_r1i1p1f1_gn_185001-201412.nc\n</code></pre> or shorter  <pre><code>source = $ILAMB_ROOT/MODELS/r1i1p1f1/Amon/rsus/gn/files/d20191115/rsus_Amon_ACCESS-ESM1-5_historical_r1i1p1f1_gn_185001-201412.nc\n</code></pre></p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#52-portable-batch-system-pbs-jobs-on-nci","title":"5.2 Portable Batch System (PBS) jobs on NCI","text":"<p>The following default PBS file, let's call it <code>ilamb_test.sh</code> can help you to setup your own, while making sure to use the correct project (#PBS -P) to charge your computing cost to: <pre><code>#!/bin/bash\n\n#PBS -N default_ilamb\n#PBS -P tm70\n#PBS -q normalbw\n#PBS -l ncpus=1\n#PBS -l mem=32GB           \n#PBS -l jobfs=10GB        \n#PBS -l walltime=00:10:00  \n#PBS -l storage=gdata/xp65+gdata/kj13+gdata/fs38\n#PBS -l wd\n\nmodule use /g/data/xp65/public/modules\nmodule load conda/access-med-0.1\n\nexport ILAMB_ROOT=$PWD/ILAMB_ROOT\nexport CARTOPY_DATA_DIR=/g/data/xp65/public/apps/cartopy-data\n\nilamb-run --config cmip.cfg --model_setup $PWD/modelroute.txt --regions global\n</code></pre></p> <p>If you are not familiar with PBS jobs on NCI, you could find the guide here. In brief: this PBS script (which you can submit via the bash command <code>qsub ilamb_test.sh</code>), will submit a job to Gadi with the job name (#PBS -N) default_ilamb under project (#PBS -P) <code>tm70</code> with a normal queue (#PBS -q normalbw), for 1 CPU (#PBS -l ncpus=1) with 32 GB RAM (#PBS -l mem=32GB), with an walltime of 10 hours and access to 10 GB local disk space (#PBS -l jobfs=10GB) as well as data storage access to projects <code>xp65</code>, <code>kj13</code>, and <code>fs38</code> (again, note that you have to be member of both projects on NCI. Upon starting the job, it will change into to the working directory that you started the job from (#PBS -l wd) and load the access-med conda environment. Finally, it will export the $ILAMB_ROOT as well as $ARTOPY_DATA_DIR paths and start an <code>ilamb-run</code>.</p> <p>In our example, we actually run the <code>cmip.cfg</code> file from the <code>ilamb</code> config file github repository for files spec</p> <p>Note: If your ILAMB_ROOT and CARTOPY_DATA_DIR are not in your directory from where you submitted the job from, then you need to adjust the export commands to their path <pre><code>export ILAMB_ROOT=/absolute/path/where/ILAMB_ROOT/actually/is\nexport CARTOPY_DATA_DIR=/absolute/path/where/shapefiles/actually/are\n</code></pre></p> <p>Once the jobs are finished, you can again inspect the outcome as described in Section 4.3</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_ilamb/#6-fix-your-setup-with-ilamb_doctor","title":"6. Fix your setup with ilamb_doctor","text":"<p><code>ilamb_doctor</code> is a script you can use to diagnosing some missing model values or what is incorrect or missing from a given analysis. It takes options similar to <code>ilamb-run</code> and is used in the following way: ```[ILAMB/test]$ ilamb-doctor --config test.cfg --model_root ${ILAMB_ROOT}/MODELS/CLM</p> <p>Searching for model results in /Users/ncf/ILAMB//MODELS/CLM</p> <pre><code>                               CLM40n16r228\n                               CLM45n16r228\n                               CLM50n18r229\n</code></pre> <p>We will now look in each model for the variables in the ILAMB configure file you specified (test.cfg). The color green is used to reflect which variables were found in the model. The color red is used to reflect that a model is missing a required variable.</p> <pre><code>                       Biomass/GlobalCarbon CLM40n16r228 biomass or cVeg\n                                    ... (abbreviated)\n                        Precipitation/GPCP2 CLM50n18r229 pr\n</code></pre> <p>``` Here we have run the command on some inputs in our test directory. You will see a list of the confrontations we run and the variables which are required or their synonyms. What is missing in this tutorial is the text coloring which will indicate if a given model has the required variables.</p> <p>We have finish the introduction of basic <code>ilamb</code> usage. We believe you have some understanding of <code>ilamb</code> and cont wait to use it. if you still have any question or you want some developer level support, you can find more detail in their official tutorial.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_pangeo_cosima/","title":"COSIMA cookbooks on NCI's Gadi","text":"<p>COSIMA is the Consortium for Ocean-Sea Ice Modelling in Australia, which brings together Australian researchers involved in global ocean and sea ice modelling. The consortium provides a collection of <code>cosmia-recipes</code> for the evaluation of ocean-sea ice modelling that are currated for you on Gadi.</p>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_pangeo_cosima/#getting-started","title":"Getting Started","text":"<p>The easiest way to use the COSIMA Cookbook is through NCI's HPC systems. The cookbook is preinstalled both in the <code>conda/analysis3</code> (project hh5) environments.</p> <ol> <li>Clone the <code>cosima-recipes</code> repository to your local file space.  </li> <li>Start an ARE JupyterLab session on NCI or a jupyter notebook on Gadi: If you are using ARE: Storage: gdata/hh5+gdata/ik11 Module directories: /g/data/hh5/public/modules Modules: conda/analysis3</li> <li>Navigate to one of the COSIMA recipes and run the analysis.</li> </ol>"},{"location":"model_evaluation/model_evaluation_on_gadi/model_evaluation_on_gadi_pangeo_cosima/#using-the-cookbook","title":"Using the Cookbook","text":"<p>The COSIMA Cookbook is a framework for analysing output from ocean-sea ice models. The focus is on the ACCESS-OM2 suite of models being developed and run by members of COSIMA: Consortium for Ocean-Sea Ice Modelling in Australia. But this framework is suited to analysing any MOM5/MOM6 output, as well as output from other models.</p> <p>The cookbook is structured as follows:     * This repository includes boiler-plate code and scripts that underpin the cookbook.     * The cosima-recipes repository includes example notebooks on which you can base your analyses, including a collection of useful examples.     * The cosima-recipes template provides you with a template if you want to contribute your own scripts to the analysis.</p>"},{"location":"models/","title":"Supported ACCESS Models","text":"<p>ACCESS is a family of related computer models that are able to represent different parts of the Earth system trough the deployment of various model components. ACCESS models link these model components through software called couplers to form different Model Configurations.</p>"},{"location":"models/#supported-access-model-configurations","title":"Supported ACCESS Model Configurations","text":"ACCESS-CM                      ACCESS Coupled Model (CM) produces physical climate simulations by deploying the atmosphere, ocean, and sea-ice components. ACCESS-CM features improved fluid dynamics and a microphysical aerosol scheme.                  ACCESS-ESM                      ACCESS Earth System Model (ESM) simulates the carbon and other bio-chemical cycles within the climate system, by deploying the atmosphere, ocean, and sea-ice components. ACCESS-ESM is one of the two ACCESS global coupled model versions.                  ACCESS-OM                      ACCESS Ocean Model (OM) deploys the ocean and sea-ice components to provide the Australian climate community with ocean weather and climate data, including seasonal forecasting, climate variability, downscaling of climate in the marine environment around Australia, and ocean biogeochemistry."},{"location":"models/#access-model-components","title":"ACCESS Model Components","text":"Atmosphere Land Ocean Sea Ice Aerosols Atmospheric Chemistry Biogeochemistry Land Biogeochemistry Ocean Coupler"},{"location":"models/configurations/","title":"Supported ACCESS Model Configurations","text":"ACCESS-CM                      ACCESS Coupled Model (CM) produces physical climate simulations by deploying the atmosphere, ocean, and sea-ice components. ACCESS-CM features improved fluid dynamics and a microphysical aerosol scheme.                  ACCESS-ESM                      ACCESS Earth System Model (ESM) simulates the carbon and other bio-chemical cycles within the climate system, by deploying the atmosphere, ocean, and sea-ice components. ACCESS-ESM is one of the two ACCESS global coupled model versions.                  ACCESS-OM                      ACCESS Ocean Model (OM) deploys the ocean and sea-ice components to provide the Australian climate community with ocean weather and climate data, including seasonal forecasting, climate variability, downscaling of climate in the marine environment around Australia, and ocean biogeochemistry."},{"location":"models/configurations/access-am/","title":"ACCESS-AM","text":"<p>The ACCESS-AM model is a coupled model between the atmosphere and the land. The atmospheric model component is the UM model. The UM model comes by default coupled to the JULES land model. That is why the first configurations and experiments released of ACCESS-AM will be UM-JULES configurations. But the ACCESS-NRI is working to ensure subsequent releases of ACCESS-AM use the CABLE land model instead.</p>"},{"location":"models/configurations/access-am/#getting-started-information","title":"Getting started information","text":"<p>On this page, you will find information on how to gain access to the UM model and start using the model. You will also find links to various configurations and experiments you can use as a basis to design your experiment.</p>"},{"location":"models/configurations/access-am/#configurations","title":"Configurations","text":""},{"location":"models/configurations/access-am/#experiments","title":"Experiments","text":"<p>Some experiments already run with the UM are listed on:</p> <ul> <li>CLEX CMS wiki</li> </ul>"},{"location":"models/configurations/access-cm/","title":"ACCESS-CM","text":"<p>ACCESS-CM2 (ACCESS Coupled Model 2) is a global fully-coupled climate model that includes the atmosphere, ocean and sea-ice components, and produces physical climate simulations. ACCESS-CM2 is one of the two models run by the Australian climate community for the   Coupled Model Intercomparison Project, CMIP. </p> <p></p>"},{"location":"models/configurations/access-cm/#access-cm2-configurations","title":"ACCESS-CM2 configurations","text":"<ul> <li> <p>Atmosphere model (UM10.6): N96 resolution (1.875\u00b0 x 1.25\u00b0, 85 levels). Physical model only \u2013 no carbon cycle.</p> </li> <li> <p>Land surface model (CABLE2.5) </p> </li> <li> <p>Ocean model (MOM5): Tripolar grid, 1\u00b0 resolution, 50 levels.</p> </li> <li> <p>Sea ice model (CICE5.1) </p> COMPONENT MODEL VERSION Atmosphere UM 10.6 Land Surface CABLE 2.5 (integrated in UM) Ocean MOM 5 Sea Ice CICE 5.1 Coupler OASIS-MCT 3 </li> </ul> <p>ACCESS-NRI will release an ACCESS-CM model configuration. The first release of ACCESS-CM will be derived from the CSIRO ACCESS-CM2 configuration and will include atmosphere, land, ocean and sea ice components.</p>"},{"location":"models/configurations/access-cm/#access-cm2","title":"ACCESS-CM2","text":"<p>Citation 1 | Tutorial</p> <p>ACCESS-CM2 1 is one of Australia\u2019s contributions to the World Climate Research Programme\u2019s Coupled Model Intercomparison Project Phase 6 (CMIP6). The component models are: UM10.6 GA7.1 for the atmosphere, CABLE2.5 for the land surface, MOM5 for the ocean, and CICE5.1.2 for the sea ice. Compared to previous model versions ACCESS-CM2 shows better global hydrological balance, more realistic ocean water properties (in terms of spatial distribution) and meridional overturning circulation in the Southern Ocean but a poorer simulation of the Antarctic sea ice and a larger energy imbalance at the top of atmosphere. This energy imbalance reflects a noticeable warming trend of the global ocean over the spin-up period.</p> <ol> <li> <p>Daohua Bi, Martin Dix, Simon Marsland, Siobhan O'Farrell, Arnold Sullivan, Roger Bodman, Rachel Law, Ian Harman, Jhan Srbinovsky, Harun A Rashid, Peter Dobrohotoff, Chloe Mackallah, Hailin Yan, Anthony Hirst, Abhishek Savita, Fabio Boeira Dias, Matthew Woodhouse, Russell Fiedler, and Aidan Heerdegen. Configuration and spin-up of ACCESS-CM2, the new generation Australian Community Climate and Earth System Simulator coupled model. Journal of Southern Hemisphere Earth Systems Science, 70(1):225\u2013251, 2020.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"models/configurations/access-esm/","title":"ACCESS-ESM","text":"<p>ACCESS-ESM stands for ACCESS Earth System Model. Earth system model means it is a fully-coupled model that includes carbon cycle components.</p> <p>ACCESS-NRI will release an ACCESS-ESM model configuration. The first release of ACCESS-ESM will be derived from the CSIRO ACCESS-ESM1.5 configuration and will include atmosphere, land and land biogeochemistry, ocean and ocean biogeochemistry, and sea ice components.</p>"},{"location":"models/configurations/access-esm/#access-esm15","title":"ACCESS-ESM1.5","text":"<p>Citation 1</p> <p>ACCESS Training Workshop (AMOS 2021)</p> <p>Webinar: Getting Started with ACCESS-CM2 and ACCESS-ESM1.5 </p> <p>ACCESS-ESM1.5 1 is a fully-coupled climate model with land and ocean carbon cycle components. ACCESS-ESM1.5 has mainly been developed to enable Australia to participate in the Coupled Model Intercomparison Project Phase 6 (CMIP6) with an ESM version. An assessment of the climate response to CO2 forcing indicates that ACCESS-ESM1.5 has an equilibrium climate sensitivity of 3.87\u00b0C.</p> <ol> <li> <p>Tilo Ziehn, Matthew A Chamberlain, Rachel M Law, Andrew Lenton, Roger W Bodman, Martin Dix, Lauren Stevens, Ying-Ping Wang, and Jhan Srbinovsky. The Australian Earth System Model: ACCESS-ESM1.5. Journal of Southern Hemisphere Earth Systems Science, 70(1):193\u2013214, 2020.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"models/configurations/access-om/","title":"ACCESS-OM","text":"<p>The ACCESS Ocean Model, ACCESS-OM, is a global coupled ocean and sea ice configuration. It couples the ocean and sea ice components via a coupler. The atmospheric fields that drive the model are provided by a data product, usually derived from reanalysis.</p> <p>ACCESS-NRI will release supported ACCESS-OM configurations. The first release will be derived from the COSIMA ACCESS-OM2 suite and will include ocean and sea ice components.</p>"},{"location":"models/configurations/access-om/#access-om2","title":"ACCESS-OM2","text":"<p>Citation 1 | Documentation</p> <p>ACCESS-OM2 1 is a suite of coupled ocean-sea ice models developed by the Consortium for Ocean-Sea Ice Modelling in Australia (COSIMA). All models use the MOM5 ocean model coupled to the CICE5 sea ice model via an OASIS3-MCT coupler.</p> <p>The models in the ACCESS-OM2 suite differ by their grid spatial resolution:</p> <ul> <li>ACCESS-OM2 at 1\u00b0 with 50 vertical levels</li> <li>ACCESS-OM2-025 at 0.25\u00b0 with 50 vertical levels</li> <li>ACCESS-OM2-01 at 0.1\u00b0 with 75 vertical levels</li> </ul> <ol> <li> <p>A. E. Kiss, A. McC. Hogg, N. Hannah, F. Boeira Dias, G. B. Brassington, M. A. Chamberlain, C. Chapman, P. Dobrohotoff, C. M. Domingues, E. R. Duran, M. H. England, R. Fiedler, S. M. Griffies, A. Heerdegen, P. Heil, R. M. Holmes, A. Klocker, S. J. Marsland, A. K. Morrison, J. Munroe, M. Nikurashin, P. R. Oke, G. S. Pilo, O. Richet, A. Savita, P. Spence, K. D. Stewart, M. L. Ward, F. Wu, and X. Zhang. ACCESS-OM2 v1.0: a global ocean\u2013sea ice model at three resolutions. Geoscientific Model Development, 13(2):401\u2013442, 2020. URL: https://gmd.copernicus.org/articles/13/401/2020/, doi:10.5194/gmd-13-401-2020.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"models/model_components/","title":"Model Components","text":"<p>ACCESS model components represent different chemical, physical or biological parts of the Earth System.</p> Atmosphere Land Ocean Sea Ice Aerosols Atmospheric Chemistry Biogeochemistry Land Biogeochemistry Ocean Coupler <p>Most of these model components have originated from collaborations with international research groups, such as:</p> <ul> <li>UK Met Office: UM atmospheric model.</li> <li>NOAA/ GFDL: MOM ocean model.</li> <li>LANL: CICE sea ice model.</li> <li>CERFACS: OASIS3-MCT coupling software package.</li> <li>UKCA: UKCA atmospheric chemistry-aerosol model.</li> <li>CSIRO, COSIMA, CLEX: CABLE land surface model, WOMBAT ocean biogeochemistry package and CASA land biogeochemistry model; all developed in Australia.</li> </ul>"},{"location":"models/model_components/aerosols_atmospheric_chemistry/","title":"Aerosol and Atmospheric Chemistry Components","text":""},{"location":"models/model_components/aerosols_atmospheric_chemistry/#ukca","title":"UKCA","text":"<p>The UK Chemistry-Aerosol model (UKCA) is a community atmospheric chemistry-aerosol global model  developed in the United Kingdom. It is suitable for a range of topics in climate and environmental change research.</p>"},{"location":"models/model_components/aerosols_atmospheric_chemistry/#how-is-ukca-used","title":"How is UKCA used?","text":"<p>UKCA chemistry model is enabled in ACCESS-CM2-Chem.</p>"},{"location":"models/model_components/aerosols_atmospheric_chemistry/#glomap","title":"GLOMAP","text":"<p>UKCA contains an aerosol scheme GLObal Model of Aerosol Processes (GLOMAP) that can be used independently. The multi-component, multi-modal GLOMAP model allows simulation of aerosol number, size and concentrations of individual components such as sulphate,sea salt and different types of carbon.</p>"},{"location":"models/model_components/aerosols_atmospheric_chemistry/#how-is-glomap-used","title":"How is GLOMAP used?","text":"<p>GLOMAP is used in ACCESS-CM2 and ACCESS-CM2-Chem.</p>"},{"location":"models/model_components/atmosphere/","title":"Atmospheric Model Component","text":""},{"location":"models/model_components/atmosphere/#the-unified-model-um","title":"The Unified Model (UM)","text":"<p>The Unified Model (UM) is a numerical model of the atmosphere used for both weather and climate applications, developed by the Met Office in the United Kingdom (UK). It includes solutions of the equations of atmospheric fluid dynamics with advanced parameterizations of subgrid-scale physical processes like convection, cloud formation and atmospheric radiation.</p> <p>The Unified Model gets its name because a single model is used across a wide range of both timescales (nowcasting to centennial) and spatial scales (sub km convective scale to global climate modelling).</p> <p>The UM is used by several international operational meteorology and research organizations and these contribute towards its development through the UM partnership.</p>"},{"location":"models/model_components/atmosphere/#how-is-the-um-used","title":"How is the UM used?","text":"<p>The UM Model component represents the atmosphere in many of the ACCESS Models used at regional and global scales.</p> <p>The ACCESS-CM2 climate model and ACCESS-ESM1-5 earth system model use versions of the UM as their atmospheric components.</p> <p>The Australian Bureau of Meteorology operational 12 km spatial resolution global forecasting system uses the Unified Model, as part of ACCESS for:</p> <ul> <li>Forecasting of extreme events and emergencies such as heatwaves, bushfires, cyclones, floods, coral bleaching, sea-level rise, coastal inundation and more.</li> <li>Daily and seasonal weather forecasts</li> </ul>"},{"location":"models/model_components/atmosphere/#useful-links","title":"Useful links","text":"<p>STASH register: Metadata reference for the outputs variables.</p>"},{"location":"models/model_components/bgc_land/","title":"Biogeochemistry Land","text":""},{"location":"models/model_components/bgc_land/#casa-cnp","title":"CASA-CNP","text":"<p>CASA-CNP, the Carnegie-Ames-Stanford Approach with Carbon-Nitrogen-Phosphorus, is the biogeochemical module implemented in the ACCESS land surface model CABLE. It models the dynamics of carbon pools and the dependance of carbon uptake due to nitrogen and phosphorous limitations.  </p>"},{"location":"models/model_components/bgc_land/#how-is-casa-cnp-used","title":"How is CASA-CNP used?","text":"<p>In the ACCESS-ESM1.5 model, CASA-CNP is enabled within CABLE for the simulation of the carbon cycle.</p>"},{"location":"models/model_components/bgc_ocean/","title":"Biogeochemistry Ocean","text":""},{"location":"models/model_components/bgc_ocean/#wombat","title":"WOMBAT","text":"<p>WOMBAT is the ocean carbon model (World Ocean Model of Biogeochemistry And Trophic-dynamics), developed in Australia. It calculates the carbon fluxes of the ocean, by simulating the evolution of phosphate, oxygen, dissolved inorganic carbon, alkalinity and iron with one class of phytoplankton and zooplankton.</p> <p>WOMBAT is a Nutrient, Phytoplankton, Zooplankton and Detritus (NPZD) model, with one zooplankton and one phytoplankton class.</p>"},{"location":"models/model_components/bgc_ocean/#how-is-wombat-used","title":"How is WOMBAT used?","text":"<p>WOMBAT is coupled to the MOM5 ocean model in the ACCESS-ESM1.5 and ACCESS-OM2 models.</p>"},{"location":"models/model_components/coupler/","title":"Coupler","text":"<p>A coupler is a software package that allows synchronised exchanges of coupling information between numerical codes representing different components of the climate system.</p>"},{"location":"models/model_components/coupler/#oasis3-mct","title":"OASIS3-MCT","text":"<p>OASIS3-MCT is the version of the Ocean Atmosphere Sea Ice Soil (OASIS) coupler interfaced with the Model Coupling Toolkit (MCT) from the Argonne National Laboratory. OASIS3-MCT is the coupler used in the configurations:</p> <ul> <li>ACCESS-ESM1.5</li> <li>ACCESS-CM2 </li> <li>ACCESS-OM2</li> <li>ACCESS-S</li> </ul>"},{"location":"models/model_components/coupler/#nuopc-interoperability-layernuopc","title":"[NUOPC interoperability layer][NUOPC]","text":"<p>NUOPC (National Unified Operational Prediction Capability) Interoperability Layer defines conventions and a set of generic components for building coupled models using the Earth System Modeling Framework (ESMF).</p> <p>ACCESS-OM3, a configuration currently under development, uses NUOPC to couple its MOM6 and CICE6 model components as there are no respective OASIS coupling interfaces for these components.</p>"},{"location":"models/model_components/land/","title":"Land Model Components","text":""},{"location":"models/model_components/land/#cable","title":"CABLE","text":"<p>Community Atmosphere Biosphere Land Exchange (CABLE) is a land surface model, used to calculate the fluxes of momentum, energy, water and carbon between the land surface and the atmosphere. It also models the main biogeochemical cycles of the land ecosystem when used in conjunction with the CASA-CNP module. </p>"},{"location":"models/model_components/land/#how-is-cable-used","title":"How is CABLE used?","text":"<p>CABLE can be run as a standalone model, for a single location, a region or globally. Coupled to the Met Office Unified Model (UM), CABLE provides the land surface component of the ACCESS Earth System Model (ACCESS-ESM) and ACCESS Coupled Model (ACCESS-CM).</p> <p>CABLE is an open source model developed by a community of Australian climate science researchers. Registration is required to access the CABLE code repository.</p>"},{"location":"models/model_components/land/#jules","title":"JULES","text":"<p>The Joint UK Land Environment System (JULES) is a community land surface model that can be used both as a standalone model and as the land surface component in the UM model. By modelling different land surface processes (surface energy balance, hydrological cycle, carbon cycle, dynamic vegetation, etc.) and their interaction with each other, JULES provides a framework to assess the impact of modifying a particular process on the ecosystem as a whole, e.g., the impact of climate change on hydrology.</p>"},{"location":"models/model_components/ocean/","title":"Ocean Model Component","text":""},{"location":"models/model_components/ocean/#modular-ocean-model-mom","title":"Modular Ocean Model (MOM)","text":"<p>The Modular Ocean Model (MOM) is one of the ocean components of the ACCESS climate model system. Used to simulate ocean currents at both regional and global scales, MOM is an invaluable tool for studying the global ocean climate system, as well as capabilities for regional and coastal applications. </p> <p>MOM is an open source development by a consortium of scientists across several government agencies and academic institutions worldwide. </p>"},{"location":"models/model_components/ocean/#mom5","title":"MOM5","text":"<p>Source Code</p> <p>MOM5 is used in supported configurations, such as ACCESS-OM2, ACCESS-CM2 and ACCESS-ESM1.5.</p>"},{"location":"models/model_components/ocean/#mom6","title":"MOM6","text":"<p>Source Code | Tutorials</p> <p>MOM6 is the most recent version. An ACCESS-OM3 configuration is currently under development that uses MOM6 as the ocean model component.</p>"},{"location":"models/model_components/sea-ice/","title":"Sea-Ice Model Component","text":""},{"location":"models/model_components/sea-ice/#cice","title":"CICE","text":"<p>CICE is a numerical model for simulating the growth, melting and movement of polar sea ice. This software package was developed by researchers at Los Alamos National Laboratory team and is currently managed by the CICE Consortium, an international group of institutions formed to maintain and develop CICE in the public domain.</p> <p>CICE5 is the current version used in ACCESS model configurations.</p> <p>CICE6 is currently under development.</p>"},{"location":"models/run-a-model/","title":"Run a Model","text":"<p>Here, we are providing the information to run different ACCESS models.</p> <p>If Model, Model Component or Model Configuration are not familiar terms for you, please check out our Model overview.</p> <p>If you have not run a model before, our Getting Started Guide will give you the basics to access the Model infrastructure on the high-performance-computing facility Gadi@NCI.</p> <p>Detailed guides for the different Model configurations can then be found on the following pages: -  Run ACCESS-ESM for the ACCESS Earth System Model configurations -  Run ACCESS-CM for the ACCESS Coupled Model configurations -  Run ACCESS-AM for the ACCESS Atmosphere Model configurations -  Run ACCESS-OM for the ACCESS Ocean Model configurations  </p>"},{"location":"models/run-a-model/run-access-cm/","title":"Run ACCESS-CM","text":""},{"location":"models/run-a-model/run-access-cm/#run-access-cm","title":"Run ACCESS-CM","text":""},{"location":"models/run-a-model/run-access-cm/#requirements","title":"Requirements","text":"<p>Before running ACCESS-CM, you need to make sure to possess the right tools and to have an account with specific institutions.</p>"},{"location":"models/run-a-model/run-access-cm/#general-requirements","title":"General requirements","text":"<p>For the general requirements needed to run all ACCESS models, please refer to the Getting Started (TO DO check link) page.</p>"},{"location":"models/run-a-model/run-access-cm/#model-specific-requirements","title":"Model-specific requirements","text":"<ul> <li> Join the access project at NCI          To join the access project at NCI, request membership for it on the access NCI project page.                  For more information on how to join specific NCI projects, please refer to How to connect to a project.     </li> <li> Connection to accessdev          To run ACCESS-CM you need the connection to accessdev, an NCI server providing configuration and run control for ACCESS-CM.                  Also, you need to make sure there is correct communication between accessdev and Gadi.                  To complete these steps, you can follow the guides on SSH connections on accessdev.     </li> <li> Get a MOSRS account          The Met Office Science Repository Service (MOSRS) is a server run by the UK Met Office (UKMO) to support collaborative development with other partners organisations, which contains the source code and configurations of some of the components used by ACCESS-CM (for example the UM).                  To apply for a MOSRS account, please contact your local institutional sponsor.     </li> </ul>"},{"location":"models/run-a-model/run-access-cm/#get-access-cm-suite","title":"Get ACCESS-CM suite","text":"<p>ACCESS-CM is a set of submodels (e.g. UM, MOM, CICE, CABLE, OASIS) with a range of model parameters, input data, and computer related information, that need to be packaged together as a suite in order to run.  Each ACCESS-CM suite has an ID, in the format <code>u-&lt;suite-name&gt;</code>, with <code>&lt;suite-name&gt;</code> being a unique identifier (e.g. <code>u-br565</code> is the CMIP6 release preindustrial experiment suite).  Typically, an existing suite is copied and then edited as needed for a particular run.</p>"},{"location":"models/run-a-model/run-access-cm/#copy-access-cm-suite-with-rosie","title":"Copy ACCESS-CM suite with Rosie","text":"<p>Rosie is an SVN repository wrapper with a set of options to work with ACCESS-CM suites.  To copy an existing suite, on accessdev:</p> <ol> <li>         Run         <pre><code>mosrs-auth</code></pre>          to authenticate using your MOSRS credentials:          mosrs-auth Please enter the MOSRS password for &lt;MOSRS-username&gt;: Successfully authenticated with MOSRS as &lt;MOSRS-username&gt; </li> <li>         Run          <pre><code>rosie checkout &lt;suite-ID&gt;</code></pre>         to create a local copy of the <code>&lt;suite-ID&gt;</code> from the UKMO repository (used mostly for testing and examining existing suites):          rosie checkout &lt;suite-ID&gt; [INFO] create: /home/565/&lt;$USER&gt;/roses [INFO] &lt;suite-ID&gt;: local copy created at /home/565/&lt;$USER&gt;/roses/&lt;suite-ID&gt;          Alternatively, run          <pre><code>rosie copy &lt;suite-ID&gt;</code></pre>         to create a new full copy (local and remote in the UKMO repository) rather than just a local copy. When a new suite is created in this way, a new unique name is generated within the repository, and populated with some descriptive information about the suite along with all the initial configuration details:          rosie copy &lt;suite-ID&gt; Copy \"&lt;suite-ID&gt;/trunk@&lt;trunk-ID&gt;\" to \"u-?????\"? [y or n (default)] y [INFO] &lt;new-suite-ID&gt;: created at https://code.metoffice.gov.uk/svn/roses-u/&lt;suite-n/a/m/e/&gt; [INFO] &lt;new-suite-ID&gt;: copied items from &lt;suite-ID&gt;/trunk@&lt;trunk-ID&gt; [INFO] &lt;suite-ID&gt;: local copy created at /home/565/&lt;$USER&gt;/roses/&lt;new-suite-ID&gt; </li> </ol> <p>For additional <code>rosie</code> options, run </p> <pre><code>rosie help</code></pre> <p> The suites are created in the user's accessdev home directory, under <code>~/roses/&lt;suite-ID&gt;</code>.  The suite directory usually contains 2 subdirectories and 3 files:</p> <ul> <li><code>app</code> \u2192 directory containing the configuration files for the various tasks within the suite.</li> <li><code>meta</code> \u2192 directory containing the GUI metadata.</li> <li><code>rose-suite.conf</code> \u2192 the main suite configuration file.</li> <li><code>rose-suite.info</code> \u2192 suite information file.</li> <li><code>suite.rc</code> \u2192 the Cylc control script file (Jinja2 language).</li> ls ~/roses/&lt;suite-ID&gt; app meta rose-suite.conf rose-suite.info suite.rc </ul>"},{"location":"models/run-a-model/run-access-cm/#edit-access-cm-suite-configuration","title":"Edit ACCESS-CM suite configuration","text":""},{"location":"models/run-a-model/run-access-cm/#rose","title":"Rose","text":"<p>Rose is a configuration editor which can be used to view, edit, or run an ACCESS-CM suite.  To edit a suite configuration, on accessdev: From inside the relevant suite directory (e.g. <code>~/roses/&lt;suite-ID&gt;</code>), run </p> <pre><code>rose edit &amp;</code></pre> <p>to open the Rose GUI and inspect the suite information. </p>      The <code>&amp;</code> is optional and keeps the terminal prompt active while runs the GUI as a separate process.  <p> cd ~/roses/&lt;suite-ID&gt; rose edit &amp; [&lt;N&gt;] &lt;PID&gt; </p>"},{"location":"models/run-a-model/run-access-cm/#change-nci-project","title":"Change NCI project","text":"<p>To make sure we run the suite under the NCI project we belong to, we can navigate to suite conf \u2192 Machine and Runtime Options, edit the Compute project field, and click the Save button . (Check how to connect to a project if you have not joined one yet).  For example, to run ACCESS-CM under the <code>tm70</code> project (ACCESS-NRI), we will insert <code>tm70</code> in the Compute project field:  </p>      You should be part of a project with allocated Service Units (SU) to be able to run ACCESS-CM. For more information please check (TO DO reference projects)."},{"location":"models/run-a-model/run-access-cm/#change-run-length-and-cycling-frequency","title":"Change run length and cycling frequency","text":"<p>ACCESS-CM suites are often run in multiple steps, each of them constituting a cycle, with the job scheduler resubmitting the suite every chosen Cycling frequency, until the Total Run length is met.  To modify these parameters, we can navigate to suite conf \u2192 Run Initialisation and Cycling, edit the respective fields, and click the Save button . The values are in the ISO 8601 Duration format.  If, for example, we want to run the suite for a total of 50 Years, and resubmit every year, we will change Total Run length to <code>P50Y</code> and Cycling frequency to <code>P1Y</code>. Note that the current maximum Cycling frequency is 2 years:  </p>"},{"location":"models/run-a-model/run-access-cm/#change-wallclock-time","title":"Change wallclock time","text":"<p>The Wallclock time is the time requested by the PBS job to run a single cycle. If this time is not enough for the suite to end its cycle, our job will be terminated before the suite can complete the run.   If we change the Cycling frequency, we might need to change the Wallclock time accordingly.   The time needed for the suite to run a full cycle depends on several factors, but a good estimation can be 4 hours per simulated year.  To modify the Wallclock time, we can navigate to suite conf \u2192 Run Initialisation and Cycling, edit the respective field, and click the Save button . The value is in the ISO 8601 Duration format.</p>"},{"location":"models/run-a-model/run-access-cm/#run-access-cm-suite","title":"Run ACCESS-CM suite","text":"<p>ACCESS-CM suites run on Gadi through a PBS job submission.  When the suite gets run, its configuration files are copied on Gadi under <code>/scratch/$PROJECT/$USER/cylc-run/&lt;suite-ID&gt;</code>, and a symbolic link to this folder is also created in the <code>$USER</code>'s home directory under <code>~/cylc-run/&lt;suite-ID&gt;</code>.  An ACCESS-CM suite is constituted by several tasks (such as checking out code repositories, compiling and building the different model components, running the model, etc.). The workflow of these tasks is controlled by Cylc.</p>"},{"location":"models/run-a-model/run-access-cm/#cylc","title":"Cylc","text":"<p>Cylc (pronounced \u2018silk\u2019), is a workflow manager that automatically executes tasks according to the model main cycle script <code>suite.rc</code>. Cylc deals with how the job will be run and manages the time steps of each submodel, as well as monitoring all the tasks and reporting any error that might occur.  To run an ACCESS-CM suite, on accessdev:</p> <ol> <li>         From inside the suite directory, run         <pre><code>rose suite-run</code></pre> </li> <li>         After the initial tasks get executed, the Cylc GUI will open up and you will be able to see and control all the different tasks in the suite as they are run:     </li> cd ~/roses/&lt;suite-ID&gt; rose suite-run [INFO] export CYLC_VERSION=7.8.3 [INFO] export ROSE_ORIG_HOST=accessdev.nci.org.au [INFO] export ROSE_SITE= [INFO] export ROSE_VERSION=2019.01.2 [INFO] create: /home/565/&lt;$USER&gt;/cylc-run/&lt;suite-ID&gt; [INFO] create: log.&lt;timestamp&gt; [INFO] symlink: log.&lt;timestamp&gt; &lt;= log [INFO] create: log/suite [INFO] create: log/rose-conf [INFO] symlink: rose-conf/&lt;timestamp&gt;-run.conf &lt;= log/rose-suite-run.conf [INFO] symlink: rose-conf/&lt;timestamp&gt;-run.version &lt;= log/rose-suite-run.version [INFO] install: rose-suite.info \u2003\u2003\u2003\u2003source: /home/565/&lt;$USER&gt;/roses/&lt;suite-ID&gt;/rose-suite.info [INFO] create: app [INFO] install: app \u2003\u2003\u2003\u2003source: /home/565/&lt;$USER&gt;/roses/&lt;suite-ID&gt;/app [INFO] create: meta [INFO] install: meta \u2003\u2003\u2003\u2003source: /home/565/&lt;$USER&gt;/roses/&lt;suite-ID&gt;/meta [INFO] install: suite.rc [INFO] REGISTERED &lt;suite-ID&gt; -&gt; /home/565/&lt;$USER&gt;/cylc-run/&lt;suite-ID&gt; [INFO] create: share [INFO] install: share [INFO] create: work [INFO] chdir: log/ [INFO] \u2003\u2003\u2003\u2003\u2003\u2003\u2009\u2009._. [INFO] \u2003\u2003\u2003\u2003\u2003\u2003\u2009\u2009| |\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003The Cylc Suite Engine [7.8.3] [INFO] ._____._. ._| |_____.\u2003\u2003\u2003\u2003\u2003\u2009Copyright (C) 2008-2019 NIWA [INFO] | .___| | | | | .___|\u2003&amp; British Crown (Met Office) &amp; Contributors. [INFO] | !___| !_! | | !___. _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [INFO] !_____!___. |_!_____! This program comes with ABSOLUTELY NO WARRANTY; [INFO] \u2003\u2003\u2003\u2009.___! | \u2003\u2003\u2003\u2003\u2003see `cylc warranty`. \u2009It is free software, you [INFO] \u2003\u2003\u2003\u2009!_____! \u2003\u2003\u2003\u2003\u2003\u2009are welcome to redistribute it under certain [INFO] [INFO] *** listening on https://accessdev.nci.org.au:&lt;port&gt;/ *** [INFO] [INFO] To view suite server program contact information: [INFO]  $ cylc get-suite-contact &lt;suite-ID&gt; [INFO] [INFO] Other ways to see if the suite is still running: [INFO]  $ cylc scan -n '&lt;suite-ID&gt;' accessdev.nci.org.au [INFO]  $ cylc ping -v --host=accessdev.nci.org.au &lt;suite-ID&gt; [INFO]  $ ps -opid,args &lt;PID&gt;  # on accessdev.nci.org.au          If after you run the command <code>rose suite-run</code> you get an error similar to the following:         <pre><code>[FAIL] Suite \"&lt;suite-ID&gt;\" appears to be running:\n            [FAIL] Contact info from: \"/home/565/&lt;$USER&gt;/cylc-run/&lt;suite-ID&gt;/.service/contact\"\n            [FAIL] \u2003\u2003CYLC_SUITE_HOST=accessdev.nci.org.au\n            [FAIL] \u2003\u2003CYLC_SUITE_OWNER=&lt;$USER&gt;\n            [FAIL] \u2003\u2003CYLC_SUITE_PORT=&lt;port&gt;\n            [FAIL] \u2003\u2003CYLC_SUITE_PROCESS=&lt;PID&gt; python2 /usr/local/cylc/cylc-7.8.3/bin/cylc-run &lt;suite-ID&gt;\n            [FAIL] Try \"cylc stop '&lt;suite-ID&gt;'\" first?\n        </code></pre>         you should run         <pre><code>rm /home/565/&lt;$USER&gt;/cylc-run/&lt;suite-ID&gt;/.service/contact</code></pre>         before running the <code>rose suite-run</code> command again.      </ol> <p>You are done!!  If you don't get any errors, you will be able to check the suite output files after the run is complete.  Note that, at this stage, it is possible to close the Cylc GUI.  If you want to open it again, from inside the suite directory run</p> <pre><code>rose suite-gcontrol</code></pre>"},{"location":"models/run-a-model/run-access-cm/#monitor-access-cm-runs","title":"Monitor ACCESS-CM runs","text":""},{"location":"models/run-a-model/run-access-cm/#check-for-errors","title":"Check for errors","text":"<p>It is quite common, especially during the first few runs, to experience errors and job failures. An ACCESS-CM suite is constituted by several tasks, and each of these tasks could fail. When a task fails, the suite is halted and you will see a red icon next to the respective task name in the Cylc GUI.   To investigate the cause of a failure, we need to look at the logs (<code>job.err</code> and <code>job.out</code>) from the suite run. There are two main ways to do so:</p> <ul> <li> Using the Cylc GUI          Right-click on the task that failed and click on View Job Logs (Viewer) \u2192 job.err or job.out.                  To access the specific task you might have to click on the arrow next to the task, to extend the drop-down menu with all the sub-taks.          </li> <li> Through the suite directory          The suite logs directories are stored inside <code>~/cylc-run/&lt;suite-ID&gt;</code> as <code>log.&lt;TIMESTAMP&gt;</code>, with the lastest set of logs also symlinked in the <code>~/cylc-run/&lt;suite-ID&gt;/log</code> directory.                  The logs for the main job are inside the <code>~/cylc-run/&lt;suite-ID&gt;/log/job</code> directory.                  Logs are separated into simulation cycles through their starting dates, and then differentiated by task.                  They are then further separated into \"attempts\" (consecutive failed/successful tasks), with <code>NN</code> being a symlink to the most recent attempt.                  In our example, the failure occurred for the 09500101 simulation cycle (starting date on 1st January 950) in the coupled task. Therefore, the directory where to find the <code>job.err</code> and <code>job.out</code> files is <code>~/cylc-run/&lt;suite-ID&gt;/log/job/09500101/coupled/NN</code>.          cd ~/cylc-run/&lt;suite-ID&gt; ls app cylc-suite.db log log.20230530T051952Z meta rose-suite.info share suite.rc suite.rc.processed work cd log ls db job rose.conf rose-suite-run.conf rose-suite-run.locs rose-suite-run.log rose-suite-run.version suite suiterc cd job ls 09500101 cd 09500101 ls coupled fcm_make2_um fcm_make_um install_warm make2_mom make_mom fcm_make2_drivers fcm_make_drivers install_ancil make2_cice make_cice cd coupled ls 01 02 03 NN cd NN ls job job-activity.log job.err job.out job.status </li> </ul>"},{"location":"models/run-a-model/run-access-cm/#stop-restart-and-reload-suites","title":"Stop, restart and reload suites","text":"<p>Sometimes, you may want to control the running state of a suite.  If your Cylc GUI has been closed and you are unsure whether your suite is still running, you can scan for active suites and reopen the GUI if desired.  To scan for active suites run</p> <pre><code>cylc scan</code></pre> <p>To reopen the Cylc GUI, from inside the suite directory run</p> <pre><code>rose suite-gcontrol</code></pre> <p> cylc scan &lt;suite-ID&gt; &lt;$USER&gt;@accessdev.nci.org.au:&lt;port&gt; cd ~/roses/&lt;suite-ID&gt; rose suite-gcontrol </p>"},{"location":"models/run-a-model/run-access-cm/#stop-a-suite","title":"STOP a suite","text":"<p>To shutdown a suite in a safe manner, from inside the suite directory run</p> <pre><code>rose suite-stop -y</code></pre> <p>Alternatively, you can directly kill the PBS job(s) connected to your run. To do so:</p> <ol> <li>         Check the status of all your PBS jobs by running         <pre><code>qstat -u $USER</code></pre> </li> <li>         Delete any job related to your run with         <pre><code>qdel &lt;job-ID&gt;</code></pre> </li> </ol>"},{"location":"models/run-a-model/run-access-cm/#restart-a-suite","title":"RESTART a suite","text":"<p>There are two main ways to restart a suite:</p> <ul> <li> 'SOFT' restart          From inside the suite directory, run          <pre><code>rose suite-run --restart</code></pre>         to re-install the suite and reopen Cylc in the same state as when it was stopped (you may need to manually trigger failed tasks from the Cylc GUI).          cylc cd ~/roses/&lt;suite-ID&gt; rose suite-run --restart [INFO] export CYLC_VERSION=7.8.3 [INFO] export ROSE_ORIG_HOST=accessdev.nci.org.au [INFO] export ROSE_SITE= [INFO] export ROSE_VERSION=2019.01.2 [INFO] delete: log/rose-suite-run.conf [INFO] symlink: rose-conf/&lt;timestamp&gt;-restart.conf &lt;= log/rose-suite-run.conf [INFO] delete: log/rose-suite-run.version [INFO] symlink: rose-conf/&lt;timestamp&gt;-restart.version &lt;= log/rose-suite-run.version [INFO] chdir: log/ [INFO] \u2003\u2003\u2003\u2003\u2003\u2003\u2009\u2009._. [INFO] \u2003\u2003\u2003\u2003\u2003\u2003\u2009\u2009| |\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003The Cylc Suite Engine [7.8.3] [INFO] ._____._. ._| |_____.\u2003\u2003\u2003\u2003\u2003\u2009Copyright (C) 2008-2019 NIWA [INFO] | .___| | | | | .___|\u2003&amp; British Crown (Met Office) &amp; Contributors. [INFO] | !___| !_! | | !___. _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ [INFO] !_____!___. |_!_____! This program comes with ABSOLUTELY NO WARRANTY; [INFO] \u2003\u2003\u2003\u2009.___! | \u2003\u2003\u2003\u2003\u2003see `cylc warranty`. \u2009It is free software, you [INFO] \u2003\u2003\u2003\u2009!_____! \u2003\u2003\u2003\u2003\u2003\u2009are welcome to redistribute it under certain [INFO] [INFO] *** listening on https://accessdev.nci.org.au:&lt;port&gt;/ *** [INFO] [INFO] To view suite server program contact information: [INFO]  $ cylc get-suite-contact &lt;suite-ID&gt; [INFO] [INFO] Other ways to see if the suite is still running: [INFO]  $ cylc scan -n '&lt;suite-ID&gt;' accessdev.nci.org.au [INFO]  $ cylc ping -v --host=accessdev.nci.org.au &lt;suite-ID&gt; [INFO]  $ ps -opid,args &lt;PID&gt;  # on accessdev.nci.org.au </li> <li> 'HARD' restart          From inside the suite directory, run         <pre><code>rose suite-run --new</code></pre>         if you want to overwrite any previous runs of the suite and begin completely afresh (WARNING!! This will overwrite all existing model output and logs for the same suite).     </li> </ul>"},{"location":"models/run-a-model/run-access-cm/#reload-a-suite","title":"RELOAD a suite","text":"<p>In some cases the suite needs to be updated without necessarily having to stop it (e.g. after fixing a typo in a file). Updating an active suite is called a 'reload', where the suite is 're-installed' and Cylc is updated with the changes (this is similar to a 'soft' restart, but with the new changes installed, so you may need to manually trigger failed tasks from the Cylc GUI).  To reload a suite, from inside the suite directory run</p> <pre><code>rose suite-run --reload</code></pre>"},{"location":"models/run-a-model/run-access-cm/#access-cm-output-files","title":"ACCESS-CM output files","text":"<p>All ACCESS-CM output files (as well as work files) are available on Gadi under <code>/scratch/$PROJECT/$USER/cylc-run/&lt;suite-ID&gt;</code> (also symlinked in <code>~/cylc-run/&lt;suite-ID&gt;</code>).  While the suite is running, files move between the <code>share</code> and the <code>work</code> directories.  At the end of each cycle, model output data and restart files are moved to <code>/scratch/$PROJECT/$USER/archive/&lt;suite-name&gt;</code>.   This directory contains 2 subdirectories:</p> <ul> <li><code>history</code></li> <li><code>restart</code></li> </ul>"},{"location":"models/run-a-model/run-access-cm/#output-data","title":"Output data","text":"<p><code>/scratch/$PROJECT/$USER/archive/&lt;suite-name&gt;/history</code> is the directory where the model output data is found, separated for each model component:</p> <ul> <li> <code>atm</code> \u2192 atmosphere (UM)     </li> <li> <code>cpl</code> \u2192 coupler (OASIS3-MCT)     </li> <li> <code>ocn</code> \u2192 ocean (MOM)     </li> <li> <code>ice</code> \u2192 ice (CICE)     </li> </ul> <p>For the atmospheric output data, each file it is usually a UM fieldsfile or netCDF file, formatted as <code>&lt;suite-name&gt;a.p&lt;output-stream-identifier&gt;&lt;year&gt;&lt;month-string&gt;</code>.  In the case of the <code>u-br565</code> suite we will have:  cd /scratch/&lt;$PROJECT&gt;/&lt;$USER&gt;/archive ls br565 &lt;other-suite-name&gt; &lt;other-suite-name&gt; cd br565 ls history restart ls history/atm br565a.pd0950apr.nc br565a.pd0950aug.nc br565a.pd0950dec.nc br565a.pd0950feb.nc br565a.pd0950jan.nc br565a.pd0950jul.nc br565a.pd0950jun.nc br565a.pd0950mar.nc br565a.pd0950may.nc br565a.pd0950nov.nc br565a.pd0950oct.nc br565a.pd0950sep.nc br565a.pd0951apr.nc br565a.pd0951aug.nc br565a.pd0951dec.nc br565a.pm0950apr.nc br565a.pm0950aug.nc br565a.pm0950dec.nc br565a.pm0950feb.nc br565a.pm0950jan.nc br565a.pm0950jul.nc br565a.pm0950jun.nc br565a.pm0950mar.nc br565a.pm0950may.nc br565a.pm0950nov.nc br565a.pm0950oct.nc br565a.pm0950sep.nc br565a.pm0951apr.nc br565a.pm0951aug.nc br565a.pm0951dec.nc netCDF </p>"},{"location":"models/run-a-model/run-access-cm/#restart-files","title":"Restart files","text":"<p><code>/scratch/$PROJECT/$USER/archive/&lt;suite-name&gt;/restart</code> is the directory where the restart dumps are found, subdivided for each model component (see <code>history</code> folder above).  For the atmospheric restart files, each of them is a UM fieldsfile, formatted as <code>&lt;suite-name&gt;a.da&lt;year&gt;&lt;month&gt;&lt;day&gt;_00</code>.  In the directory there are also some files formatted as <code>&lt;suite-name&gt;a.xhist-&lt;year&gt;&lt;month&gt;&lt;day&gt;</code> containing metadata information.</p> <p> In the case of the <code>u-br565</code> suite we will have:  ls /scratch/&lt;$PROJECT&gt;/&lt;$USER&gt;/archive/br565/restart/atm br565a.da09500201_00 br565a.da09510101_00 br565.xhist-09500131 br565.xhist-09501231  </p> <p></p> References <ul> <li> https://confluence.csiro.au/display/ACCESS/Using+CM2+suites+in+Rose+and+Cylc </li> <li> https://confluence.csiro.au/display/ACCESS/Understanding+CM2+output </li> <li> https://nespclimate.com.au/wp-content/uploads/2020/10/Instruction-document-Getting_started_with_ACCESS.pdf </li> <li> https://code.metoffice.gov.uk/doc/um/latest/um-training/rose-gui.html </li> </ul>"},{"location":"models/run-a-model/run-access-esm/","title":"Run ACCESS-ESM","text":""},{"location":"models/run-a-model/run-access-esm/#requirements","title":"Requirements","text":"<p>Before running ACCESS-ESM, you need to make sure to possess the right tools and to have an account with specific institutions. </p>"},{"location":"models/run-a-model/run-access-esm/#general-requirements","title":"General requirements","text":"<p>For the general requirements needed to run all ACCESS models, please refer to the Getting Started (TO DO check link) page.</p>"},{"location":"models/run-a-model/run-access-esm/#model-specific-requirements","title":"Model-specific requirements","text":"<ul> <li> Join the hh5 project at NCI          The hh5 project hosts the conda environment that supports most workflows for climate science on Gadi.                  To join the hh5 project at NCI, request membership for it on the hh5 NCI project page.                  For more information on how to join specific NCI projects, please refer to How to connect to a project.     </li> <li> Payu payu on Gadi is available through the <code>conda/analysis3</code> environment in the hh5 project.                  After getting access to the hh5 project, load the <code>conda/analysis3</code> environment by running:         <pre><code>module use /g/data/hh5/public/modules\n            module load conda/analysis3\n        </code></pre>         to automatically get payu.                   To check that payu is effectively available, you can run:         <pre><code>payu --version</code></pre> payu --version 1.0.19 </li> </ul>"},{"location":"models/run-a-model/run-access-esm/#get-access-esm-configuration","title":"Get ACCESS-ESM configuration","text":"<p>A suitable ACCESS-ESM pre-industrial configuration is avaible on the coecms GitHub.  In order to get it, on Gadi, create a directory where to keep the model configuration, and clone the GitHub repo in it by running: </p> <pre><code>git clone https://github.com/coecms/esm-pre-industrial.git</code></pre> <p> mkdir -p ~/access-esm cd ~/access-esm git clone https://github.com/coecms/esm-pre-industrial Cloning into 'esm-pre-industrial'... remote: Enumerating objects: 767, done. remote: Counting objects: 100% (295/295), done. remote: Compressing objects: 100% (138/138), done. remote: Total 767 (delta 173), reused 274 (delta 157), pack-reused 472 Receiving objects: 100% (767/767), 461.57 KiB | 5.24 MiB/s, done. Resolving deltas: 100% (450/450), done. </p>      Some modules might interfere with the <code>git</code> commands (for example matlab/R2018a). If you are running into issues during the cloning of the repository, it might be a good idea to run <pre><code>module purge</code></pre> first, before trying again."},{"location":"models/run-a-model/run-access-esm/#edit-access-esm-configuration","title":"Edit ACCESS-ESM configuration","text":"<p>First, is good practice to create another git branch where to keep all modifications we put in place for our run, and to keep the reference configuration unmodified. If we call the local branch \"example_run\", we can run:</p> <pre><code>git checkout -b example_run</code></pre>"},{"location":"models/run-a-model/run-access-esm/#payu","title":"Payu","text":"<p>Payu is a workflow management tool for running numerical models in supercomputing environments.  The general layout of a payu-supported model run consists of two main directories:</p> <ul> <li>         The laboratory is the directory where all parts of the model are kept. For ACCESS-ESM, it is typically <code>/scratch/$PROJECT/$USER/access-esm</code>.     </li> <li>         The control directory, where the model configuration is kept and from where the model is run (in our case is the cloned directory <code>~/access-esm/esm-pre-industrial</code>).     </li> </ul> <p>This distinction of directories keeps the small-size configuration files separated from the larger binary outputs and inputs. In this way, we can place the configuration files in the <code>$HOME</code> directory (being the only filesystem on Gadi that is actively backed up), without overloading it with too much data.  Moreover, this separation allows to run multiple self-resubmitting experiments simultaneously that might share common executables and input data.  To proceed with the setup of the laboratory directory, from the control directory run:</p> <pre><code>payu init</code></pre> <p>This will create the laboratory directory, along with other subdirectories (depending on the configuration). The main subdirectories we are interested in are: </p> <ul> <li><code>work</code> \u2192 temporary directory where the model is actually run. It gets cleaned after each run.</li> <li><code>archive</code> \u2192 directory where the output is placed after each run.</li> cd ~/access-esm/esm-pre-industrial payu init laboratory path:  /scratch/$PROJECT/$USER/access-esm binary path:  /scratch/$PROJECT/$USER/access-esm/bin input path:  /scratch/$PROJECT/$USER/access-esm/input work path:  /scratch/$PROJECT/$USER/access-esm/work archive path:  /scratch/$PROJECT/$USER/access-esm/archive </ul>"},{"location":"models/run-a-model/run-access-esm/#edit-the-master-configuration-file","title":"Edit the Master Configuration file","text":"<p>The <code>config.yaml</code> file, located in the control directory, is the Master Configuration file.   This file controls the general model configuration and if we open it in a text editor, we can see different parts:</p> <ul> <li> PBS resources <pre><code>jobname: pre-industrial\n            queue: normal\n            walltime: 20:00:00\n        </code></pre>         These are settings for the PBS scheduler. Edit lines in this section to change any of the PBS resources.                   For example, to run ACCESS-ESM under the <code>tm70</code> project (ACCESS-NRI), add the following line to this section:         <pre><code>project: tm70</code></pre>              You should be part of a project with allocated Service Units (SU) to be able to run ACCESS-ESM. For more information please check (TO DO reference projects).          </li> <li> Link to the laboratory directory <pre><code># note: if laboratory is relative path, it is relative to /scratch/$PROJECT/$USER\n            laboratory: access-esm\n        </code></pre>         This will set the laboratory directory. Relative paths are relative to <code>/scratch/$PROJECT/$USER</code>. Absolute paths can be specified as well.     </li> <li> Model <pre><code>model: access</code></pre>         The main model. This tells payu which driver to use (access stands for ACCESS-ESM).     </li> <li> Submodels <pre><code>submodels:\n            \u00a0\u00a0- name: atmosphere\n            \u00a0\u00a0\u00a0\u00a0model: um\n            \u00a0\u00a0\u00a0\u00a0ncpus: 192\n            \u00a0\u00a0\u00a0\u00a0exe: /g/data/access/payu/access-esm/bin/coe/um7.3x\n            \u00a0\u00a0\u00a0\u00a0input:\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/access/payu/access-esm/input/pre-industrial/atmosphere\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/access/payu/access-esm/input/pre-industrial/start_dump\n            \u00a0\u00a0- name: ocean\n            \u00a0\u00a0\u00a0\u00a0model: mom\n            \u00a0\u00a0\u00a0\u00a0ncpus: 180\n            \u00a0\u00a0\u00a0\u00a0exe: /g/data/access/payu/access-esm/bin/coe/mom5xx\n            \u00a0\u00a0\u00a0\u00a0input:\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/access/payu/access-esm/input/pre-industrial/ocean/common\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/access/payu/access-esm/input/pre-industrial/ocean/pre-industrial\n            \u00a0\u00a0- name: ice\n            \u00a0\u00a0\u00a0\u00a0model: cice\n            \u00a0\u00a0\u00a0\u00a0ncpus: 12\n            \u00a0\u00a0\u00a0\u00a0exe: /g/data/access/payu/access-esm/bin/coe/cicexx\n            \u00a0\u00a0\u00a0\u00a0input:\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/access/payu/access-esm/input/pre-industrial/ice\n            \u00a0\u00a0- name: coupler\n            \u00a0\u00a0\u00a0\u00a0model: oasis\n            \u00a0\u00a0\u00a0\u00a0ncpus: 0\n            \u00a0\u00a0\u00a0\u00a0input:\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/access/payu/access-esm/input/pre-industrial/coupler\n        </code></pre>         ACCESS-ESM is a coupled model, which means it has multiple submodels (i.e. model components).                   This section specifies the submodels and contains configuration options (for example the directories of input files) that are required to ensure the model can execute correctly. Each submodel also has additional configuration options that are read in when the submodel is running. These specific configuration options are found in the subdirectory of the control directory having the name of the submodel (e.g. in our case the configuration for the atmosphere submodel, i.e. the UM, will be in the directory <code>~/access-esm/esm-pre-industrial/atmosphere</code>).     </li> <li> Collate <pre><code>collate:\n            \u00a0\u00a0exe: /g/data/access/payu/access-esm/bin/mppnccombine\n            \u00a0\u00a0restart: true\n            \u00a0\u00a0mem: 4GB\n        </code></pre>         The collate process joins a number of smaller files, which contain different parts of the model grid, together into target output files. The restart files are typically tiled in the same way and will also be joined together if the restart option is set to <code>true</code>.     </li> <li> Restart <pre><code>restart: /g/data/access/payu/access-esm/restart/pre-industrial</code></pre>         The location of the files used for a warm restart.     </li> <li> Start date and internal run length <pre><code>calendar:\n            \u00a0\u00a0start:\n            \u00a0\u00a0\u00a0\u00a0year: 101\n            \u00a0\u00a0\u00a0\u00a0month: 1\n            \u00a0\u00a0\u00a0\u00a0days: 1\n            \u00a0\u00a0runtime:\n            \u00a0\u00a0\u00a0\u00a0years: 1\n            \u00a0\u00a0\u00a0\u00a0months: 0\n            \u00a0\u00a0\u00a0\u00a0days: 0\n        </code></pre>         This section specifies the start date and internal run length.                       The internal run length (controlled by <code>runtime</code>) can be different from the total run length. Also, the <code>runtime</code> value can be lowered, but should not be increased to a total of more than 1 year, to avoid errors. If you want to know more about the difference between internal run and total run lenghts, or if you want to run the model for more than 1 year, check Run configuration for multiple years.          </li> <li> Number of runs per PBS submission <pre><code>runspersub: 5</code></pre>         ACCESS-ESM configurations are often run in multiple steps (or cycles), with payu running a maximum of <code>runspersub</code> internal runs for every PBS job submission.                       If we increase <code>runspersub</code>, we might need to increase the walltime in the PBS resources.          </li> </ul> <p> To know more about other configuration settings for the <code>config.yaml</code> file, please check how to configure your experiment with payu.</p>"},{"location":"models/run-a-model/run-access-esm/#run-access-esm-configuration","title":"Run ACCESS-ESM configuration","text":"<p>After editing the configuration, we are ready to run ACCESS-ESM.   ACCESS-ESM suites run on Gadi through a PBS job submission managed by payu.</p>"},{"location":"models/run-a-model/run-access-esm/#payu-setup-optional","title":"Payu setup (optional)","text":"<p>As a first step, from the control directory, is good practice to run:</p> <pre><code>payu setup</code></pre> <p>This will prepare the model run, based on the experiment configuration.  payu setup laboratory path:  /scratch/$PROJECT/$USER/access-esm binary path:  /scratch/$PROJECT/$USER/access-esm/bin input path:  /scratch/$PROJECT/$USER/access-esm/input work path:  /scratch/$PROJECT/$USER/access-esm/work archive path:  /scratch/$PROJECT/$USER/access-esm/archive Loading input manifest: manifests/input.yaml Loading restart manifest: manifests/restart.yaml Loading exe manifest: manifests/exe.yaml Setting up atmosphere Setting up ocean Setting up ice Setting up coupler Checking exe and input manifests Updating full hashes for 3 files in manifests/exe.yaml Creating restart manifest Updating full hashes for 30 files in manifests/restart.yaml Writing manifests/restart.yaml Writing manifests/exe.yaml </p>      You can skip this step as it is included also in the run command. However, runnning it explicitly helps to check for errors and make sure executable and restart directories are accessible."},{"location":"models/run-a-model/run-access-esm/#run-configuration","title":"Run configuration","text":"<p>To run ACCESS-ESM configuration for one internal run length (controlled by <code>runtime</code> in the <code>config.yaml</code> file), run:</p> <pre><code>payu run -f</code></pre> <p>This will submit a single job to the queue with a total run length of <code>runtime</code>. It there is no previous run, it will start from the <code>start</code> date indicated in the <code>config.yaml</code> file, otherwise it will perform a warm restart from a precedently saved restart file. </p>      The <code>-f</code> option ensures that payu will run even if there is an existing non-empty work directory, which happens if a run crashes.  <p> payu run -f Loading input manifest: manifests/input.yaml Loading restart manifest: manifests/restart.yaml Loading exe manifest: manifests/exe.yaml payu: Found modules in /opt/Modules/v4.3.0 qsub -q normal -P &lt;project&gt; -l walltime=11400 -l ncpus=384 -l mem=1536GB -N pre-industrial -l wd -j n -v PAYU_PATH=/g/data/hh5/public/apps/miniconda3/envs/analysis3-23.01/bin,MODULESHOME=/opt/Modules/v4.3.0,MODULES_CMD=/opt/Modules/v4.3.0/libexec/modulecmd.tcl,MODULEPATH=/g/data3/hh5/public/modules:/etc/scl/modulefiles:/opt/Modules/modulefiles:/opt/Modules/v4.3.0/modulefiles:/apps/Modules/modulefiles -W umask=027 -l storage=gdata/access+gdata/hh5 -- /g/data/hh5/public/apps/miniconda3/envs/analysis3-23.01/bin/python3.9 /g/data/hh5/public/apps/miniconda3/envs/analysis3-23.01/bin/payu-run &lt;job-ID&gt;.gadi-pbs </p>"},{"location":"models/run-a-model/run-access-esm/#run-configuration-for-multiple-years","title":"Run configuration for multiple years","text":"<p>If you want to run ACCESS-ESM configuration for multiple internal run lengths (controlled by <code>runtime</code> in the <code>config.yaml</code> file), you can use the option <code>-n</code>:</p> <pre><code>payu run -f -n &lt;number-of-runs&gt;</code></pre> <p>This will run the configuration <code>number-of-runs</code> times with a total run length of <code>runtime * number-of-runs</code>. The number of consecutive PBS jobs submitted to the queue depends on the <code>runspersub</code> value specified in the <code>config.yaml</code> file.</p>"},{"location":"models/run-a-model/run-access-esm/#understand-runtime-runspersub-and-n-parameters","title":"Understand <code>runtime</code>, <code>runspersub</code>, and <code>-n</code> parameters","text":"<p>With the correct use of <code>runtime</code>, <code>runspersub</code>, and <code>-n</code> parameters, we can have full control of our run. </p> <ul> <li> <code>runtime</code> defines the internal run length.     </li> <li> <code>runspersub</code> defines the maximum number of internal runs for every PBS job submission.     </li> <li> <code>-n</code> sets the number of internal runs to be performed.     </li> </ul> <p>Let's have some practical examples:</p> <ul> <li> Run 20 years of simulation, with resubmission every 5 years          To have a total run length of 20 years, with a resubmition cycle of 5 years, we can leave <code>runtime</code> to the default value of <code>1 year</code>, set <code>runspersub</code> to <code>5</code>, and run the configuration using <code>-n 20</code>:         <pre><code>payu run-f -n 20</code></pre>         This will submit subsequent jobs for the following years: 1 to 5, 6 to 10, 11 to 15, and 16 to 20. With a total of 4 PBS jobs.     </li> <li> Run 7 years of simulation, with resubmission every 3 years          To have a total run length of 7 years, with a resubmition cycle of 3 years, we can leave <code>runtime</code> to the default value of <code>1 year</code>, set <code>runspersub</code> to <code>3</code>, and run the configuration using <code>-n 7</code>:         <pre><code>payu run -f -n 7</code></pre>         This will submit subsequent jobs for the following years: 1 to 3, 4 to 6, and 7. With a total of 3 PBS jobs.     </li> <li> Run 3 months and 10 days of simulation, in one single submission          To have a total run length of 3 months and 10 days, all in a single submission, we have to set <code>runtime</code> to:         <pre><code>years: 0\n            months: 3\n            days: 10\n        </code></pre>         set <code>runspersub</code> to <code>1</code> (or any value &gt; 1), and run the configuration without <code>-n</code> (or with <code>-n</code> equals <code>1</code>):         <pre><code>payu run -f</code></pre> </li> <li> Run 1 year and 4 months of simulation, with resubmission every 4 months          To have a total run length of 1 year and 4 months (16 months), we will have to split it into multiple internal runs. For example, we can have 4 internal runs of 4 months each. Therefore, we will have to set <code>runtime</code> to:         <pre><code>years: 0\n            months: 4\n            days: 0\n        </code></pre>         Since the internal run length is set to 4 months, to resubmit our jobs every 4 months (i.e. every internal run), we have to set <code>runspersub</code> to <code>1</code>. Finally, we will perform 4 internal runs by running the configuration with <code>-n 4</code>:         <pre><code>payu run -f -n 4</code></pre> </li> </ul>"},{"location":"models/run-a-model/run-access-esm/#monitor-access-esm-runs","title":"Monitor ACCESS-ESM runs","text":"<p>Currently, there is no specific tool to monitor ACCESS-ESM runs.   One way to check the status of our run is running:</p> <pre><code>qstat -u $USER</code></pre> <p>This will show the status of all your PBS jobs (if there is any PBS job submitted):  qstat -u $USER Job id\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Name\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0User\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Time Use\u00a0S Queue ---------------------  ---------------- ----------------  -------- - ----- &lt;job-ID&gt;.gadi-pbs\u00a0\u00a0\u00a0\u00a0\u00a0pre-industrial\u00a0\u00a0\u00a0&lt;$USER&gt;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&lt;time&gt;\u00a0R\u00a0normal-exec &lt;job-ID-2&gt;.gadi-pbs\u00a0\u00a0\u00a0&lt;other-job-name&gt;\u00a0&lt;$USER&gt;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&lt;time&gt;\u00a0R\u00a0normal-exec &lt;job-ID-3&gt;.gadi-pbs\u00a0\u00a0\u00a0&lt;other-job-name&gt;\u00a0&lt;$USER&gt;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&lt;time&gt;\u00a0R\u00a0normal-exec  If you changed the <code>jobname</code> in the PBS resources of the Master Configuration file, that will be your job's Name instead of <code>pre-industrial</code>.  S indicates the status of your run:</p> <ul> <li>Q \u2192 Job waiting in the queue to start</li> <li>R \u2192 Job running</li> <li>E \u2192 Job ending</li> </ul> <p>If there is no listed job with your <code>jobname</code> (or if there is no job submitted at all), your run might have successfully completed, or might have been terminated due to an error.</p>"},{"location":"models/run-a-model/run-access-esm/#stop-a-run","title":"Stop a run","text":"<p>If you want to manually terminate a run, you can do it by running:</p> <pre><code>qdel &lt;job-ID&gt;</code></pre>"},{"location":"models/run-a-model/run-access-esm/#error-and-output-log-files","title":"Error and output log files","text":"<p>While the model is running, payu saves the standard output and standard error into the files <code>access.out</code> and <code>access.err</code> in the control directory. You can examine these files, as the run progresses, to check on it's status.  After the model has completed its run, or if it crashed, the output and error log files, respectively, are renamed by default into <code>jobname.o&lt;job-ID&gt;</code> and <code>jobname.e&lt;job-ID&gt;</code>.</p>"},{"location":"models/run-a-model/run-access-esm/#access-esm-outputs","title":"ACCESS-ESM outputs","text":"<p>While the configuration is running, output files (as well as restart files) are moved from the <code>work</code> directory to the <code>archive</code> directory, under <code>/scratch/$PROJECT/$USER/access-esm/archive</code> (also symlinked in the control directory under <code>~/access-esm/esm-pre-industrial/archive</code>).  Both outputs and restarts are stored into subfolders for each different configuration (<code>esm-pre-industrial</code> in our case), and inside the configuration folder, they are subdivided for each internal run.  The format of a typical output folder is <code>outputXXX</code>, whereas the typical restart folder is usually formatted as <code>restartXXX</code>, with XXX being the number of internal run, starting from <code>000</code>.  In the respective folders, outputs and restarts are separated for each model component.  For the atmospheric output data, each file it is usually a UM fieldsfile, formatted as <code>&lt;UM-suite-identifier&gt;a.p&lt;output-stream-identifier&gt;&lt;time-identifier&gt;</code>.  cd /scratch/$PROJECT/$USER/access-esm/archive/esm-pre-industrial ls output000 pbs_logs restart000 ls output000/atmosphere aiihca.daa1210 aiihca.daa1810 aiihca.paa1apr aiihca.paa1jun aiihca.pea1apr aiihca.pea1jun aiihca.pga1apr aiihca.pga1jun atm.fort6.pe0 exstat ihist prefix.CNTLGEN UAFLDS_A aiihca.daa1310  aiihca.daa1910  aiihca.paa1aug aiihca.paa1mar aiihca.pea1aug aiihca.pea1mar aiihca.pga1aug aiihca.pga1mar cable.nml fort.57 INITHIS prefix.PRESM_A um_env.py aiihca.daa1410 aiihca.daa1a10 aiihca.paa1dec aiihca.paa1may aiihca.pea1dec aiihca.pea1may aiihca.pga1dec aiihca.pga1may CNTLALL ftxx input_atm.nml SIZES xhist aiihca.daa1510 aiihca.daa1b10 aiihca.paa1feb aiihca.paa1nov aiihca.pea1feb aiihca.pea1nov aiihca.pga1feb aiihca.pga1nov CONTCNTL ftxx.new namelists STASHC aiihca.daa1610 aiihca.daa1c10 aiihca.paa1jan aiihca.paa1oct aiihca.pea1jan aiihca.pea1oct aiihca.pga1jan aiihca.pga1oct debug.root.01 ftxx.vars nout.000000 thist aiihca.daa1710 aiihca.daa2110 aiihca.paa1jul aiihca.paa1sep aiihca.pea1jul aiihca.pea1sep aiihca.pga1jul aiihca.pga1sep errflag hnlist prefix.CNTLATM UAFILES_A </p> References <ul> <li> https://github.com/coecms/esm-pre-industrial </li> <li> https://payu.readthedocs.io/en/latest/usage.html </li> </ul>"},{"location":"models/run-a-model/run-access-om/","title":"Run ACCESS-OM","text":""},{"location":"models/run-a-model/run-access-om/#requirements","title":"Requirements","text":"<p>Before running ACCESS-OM, you need to make sure to possess the right tools and to have an account with specific institutions.</p>"},{"location":"models/run-a-model/run-access-om/#general-requirements","title":"General requirements","text":"<p>For the general requirements needed to run all ACCESS models, please refer to the Getting Started (TO DO check link) page.</p>"},{"location":"models/run-a-model/run-access-om/#model-specific-requirements","title":"Model-specific requirements","text":"<ul> <li> Join the hh5, qv56, ua8 and ik11 projects at NCI          The hh5 project hosts the conda environment that supports most workflows for climate science on Gadi.                  The qv56, ua8 and ik11 projects, instead, store some of the input data for ACCESS-OM.                  To join these projects at NCI, request membership for them on the respective hh5, qv56, ua8 and ik11 NCI project pages.                  For more information on how to join specific NCI projects, please refer to How to connect to a project.     </li> <li> Payu payu on Gadi is available through the <code>conda/analysis3</code> environment in the hh5 project.                  After getting access to the hh5 project, load the <code>conda/analysis3</code> environment by running:         <pre><code>module use /g/data/hh5/public/modules\n            module load conda/analysis3\n        </code></pre>         to automatically get payu.                   To check that payu is effectively available, you can run:         <pre><code>payu --version</code></pre> payu --version 1.0.19 </li> </ul>"},{"location":"models/run-a-model/run-access-om/#get-access-om-configuration","title":"Get ACCESS-OM configuration","text":"<p>A standard ACCESS-OM configuration is avaible on the COSIMA GitHub.  This is a 1\u00b0 horizontal resolution configuration, with interannual forcing from 1 Jan 1958 to 31 Dec 2018.  In order to get it, on Gadi, create a directory where to keep the model configuration, and clone the GitHub repo in it by running: </p> <pre><code>git clone https://github.com/COSIMA/1deg_jra55_iaf.git</code></pre> <p> mkdir -p ~/access-om cd ~/access-eom git clone https://github.com/COSIMA/1deg_jra55_iaf.git Cloning into '1deg_jra55_iaf'... remote: Enumerating objects: 14715, done. remote: Counting objects: 100% (3401/3401), done. remote: Compressing objects: 100% (24/24), done. remote: Total 14715 (delta 3383), reused 3379 (delta 3377), pack-reused 11314 Receiving objects: 100% (14715/14715), 35.68 MiB | 18.11 MiB/s, done. Resolving deltas: 100% (10707/10707), done. </p>      Some modules might interfere with the <code>git</code> commands (for example matlab/R2018a). If you are running into issues during the cloning of the repository, it might be a good idea to run <pre><code>module purge</code></pre> first, before trying again."},{"location":"models/run-a-model/run-access-om/#edit-access-om-configuration","title":"Edit ACCESS-OM configuration","text":"<p>First, is good practice to create another git branch where to keep all modifications we put in place for our run, and to keep the reference configuration unmodified. If we call the local branch \"example_run\", we can run:</p> <pre><code>git checkout -b example_run</code></pre>"},{"location":"models/run-a-model/run-access-om/#payu","title":"Payu","text":"<p>Payu is a workflow management tool for running numerical models in supercomputing environments.  The general layout of a payu-supported model run consists of two main directories:</p> <ul> <li>         The laboratory is the directory where all parts of the model are kept. For ACCESS-OM, it is typically <code>/scratch/$PROJECT/$USER/access-om2</code>.     </li> <li>         The control directory, where the model configuration is kept and from where the model is run (in our case is the cloned directory <code>~/access-om/1deg_jra55_iaf</code>).     </li> </ul> <p>This distinction of directories keeps the small-size configuration files separated from the larger binary outputs and inputs. In this way, we can place the configuration files in the <code>$HOME</code> directory (being the only filesystem on Gadi that is actively backed up), without overloading it with too much data.  Moreover, this separation allows to run multiple self-resubmitting experiments simultaneously that might share common executables and input data.  To proceed with the setup of the laboratory directory, from the control directory run:</p> <pre><code>payu init</code></pre> <p>This will create the laboratory directory, along with other subdirectories (depending on the configuration). The main subdirectories we are interested in are: </p> <ul> <li><code>work</code> \u2192 temporary directory where the model is actually run. It gets cleaned after each run.</li> <li><code>archive</code> \u2192 directory where the output is placed after each run.</li> cd ~/access-om/1deg_jra55_iaf payu init laboratory path:  /scratch/$PROJECT/$USER/access-om2 binary path:  /scratch/$PROJECT/$USER/access-om2/bin input path:  /scratch/$PROJECT/$USER/access-om2/input work path:  /scratch/$PROJECT/$USER/access-om2/work archive path:  /scratch/$PROJECT/$USER/access-om2/archive </ul>"},{"location":"models/run-a-model/run-access-om/#edit-the-master-configuration-file","title":"Edit the Master Configuration file","text":"<p>The <code>config.yaml</code> file, located in the control directory, is the Master Configuration file.   This file controls the general model configuration and if we open it in a text editor, we can see different parts:</p> <ul> <li> PBS resources <pre><code>queue: normal\n            walltime: 3:00:00\n            jobname: 1deg_jra55_iaf\n            mem: 1000GB\n        </code></pre>         These are settings for the PBS scheduler. Edit lines in this section to change any of the PBS resources.                   For example, to run ACCESS-OM under the <code>tm70</code> project (ACCESS-NRI), add the following line to this section:         <pre><code>project: tm70</code></pre>              You should be part of a project with allocated Service Units (SU) to be able to run ACCESS-OM. For more information please check (TO DO reference projects).          </li> <li> Model configuration <pre><code>name: common\n            model: access-om2\n            input: /g/data/ik11/inputs/access-om2/input_20201102/common_1deg_jra55\n        </code></pre>         The main model configuration. This tells payu which driver to use (access-om).                  The <code>name</code> field here is not actually used for the configuration run and you can safely disregard it.     </li> <li> Submodels <pre><code>submodels:\n            \u00a0\u00a0- name: atmosphere\n            \u00a0\u00a0\u00a0\u00a0model: yatm\n            \u00a0\u00a0\u00a0\u00a0exe: /g/data/access/payu/access-om2/bin/coe/um7.3x\n            \u00a0\u00a0\u00a0\u00a0input:\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/ik11/inputs/access-om2/input_20201102/yatm_1deg\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/qv56/replicas/input4MIPs/CMIP6/OMIP/MRI/MRI-JRA55-do-1-4-0/atmos/3hr/rsds/gr/v20190429\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/qv56/replicas/input4MIPs/CMIP6/OMIP/MRI/MRI-JRA55-do-1-4-0/atmos/3hr/rlds/gr/v20190429\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/qv56/replicas/input4MIPs/CMIP6/OMIP/MRI/MRI-JRA55-do-1-4-0/atmos/3hr/prra/gr/v20190429\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/qv56/replicas/input4MIPs/CMIP6/OMIP/MRI/MRI-JRA55-do-1-4-0/atmos/3hr/prsn/gr/v20190429\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/qv56/replicas/input4MIPs/CMIP6/OMIP/MRI/MRI-JRA55-do-1-4-0/atmos/3hrPt/psl/gr/v20190429\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/qv56/replicas/input4MIPs/CMIP6/OMIP/MRI/MRI-JRA55-do-1-4-0/land/day/friver/gr/v20190429\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/qv56/replicas/input4MIPs/CMIP6/OMIP/MRI/MRI-JRA55-do-1-4-0/atmos/3hrPt/tas/gr/v20190429\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/qv56/replicas/input4MIPs/CMIP6/OMIP/MRI/MRI-JRA55-do-1-4-0/atmos/3hrPt/huss/gr/v20190429\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/qv56/replicas/input4MIPs/CMIP6/OMIP/MRI/MRI-JRA55-do-1-4-0/atmos/3hrPt/uas/gr/v20190429\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/qv56/replicas/input4MIPs/CMIP6/OMIP/MRI/MRI-JRA55-do-1-4-0/atmos/3hrPt/vas/gr/v20190429\n            \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- /g/data/qv56/replicas/input4MIPs/CMIP6/OMIP/MRI/MRI-JRA55-do-1-4-0/landIce/day/licalvf/gr/v20190429\n            \u00a0\u00a0\u00a0\u00a0ncpus: 1\n            \u00a0\u00a0- name: ocean\n            \u00a0\u00a0\u00a0\u00a0model: mom\n            \u00a0\u00a0\u00a0\u00a0exe: /g/data/ik11/inputs/access-om2/bin/fms_ACCESS-OM_730f0bf_libaccessom2_d750b4b.x\n            \u00a0\u00a0\u00a0\u00a0input: /g/data/ik11/inputs/access-om2/input_20201102/mom_1deg\n            \u00a0\u00a0\u00a0\u00a0ncpus: 216\n            \u00a0\u00a0- name: ice\n            \u00a0\u00a0\u00a0\u00a0model: cice\n            \u00a0\u00a0\u00a0\u00a0exe: /g/data/ik11/inputs/access-om2/bin/cice_auscom_360x300_24p_edcfa6f_libaccessom2_d750b4b.exe\n            \u00a0\u00a0\u00a0\u00a0input: /g/data/ik11/inputs/access-om2/input_20201102/cice_1deg\n            \u00a0\u00a0\u00a0\u00a0ncpus: 24\n        </code></pre>         ACCESS-OM is a coupled model, which means it has multiple submodels (i.e. model components).                   This section specifies the submodels and contains configuration options (for example the directories of input files) that are required to ensure the model can execute correctly. Each submodel also has additional configuration options that are read in when the submodel is running. These specific configuration options are found in the subdirectory of the control directory having the name of the submodel (e.g. in our case the configuration for the atmosphere submodel will be in the directory <code>~/access-om/1deg_jra55_iaf/atmosphere</code>).     </li> <li> Collate <pre><code>collate:\n            \u00a0\u00a0restart: true\n            \u00a0\u00a0walltime: 1:00:00\n            \u00a0\u00a0mem: 30GB\n            \u00a0\u00a0ncpus: 4\n            \u00a0\u00a0queue: normal\n            \u00a0\u00a0exe: /g/data/ik11/inputs/access-om2/bin/mppnccombine\n        </code></pre>         The collate process joins a number of smaller files, which contain different parts of the model grid, together into target output files. The restart files are typically tiled in the same way and will also be joined together if the restart option is set to <code>true</code>.     </li> <li> Runlog <pre><code>runlog: true</code></pre>         When running a new configuration, payu automatically commits changes in git if runlog is set to true.     </li> <li> Stack size <pre><code>stacksize: unlimited</code></pre>         The stack size is the maximum size of the per-thread resources allocated for each process (in KiB).         unlimited works without any issues, but explicit stacksize values may not be correctly communicated across Gadi nodes.     </li> <li> Restart frequency <pre><code>restart_freq: 1</code></pre>         The restart frequency specifies the rate of saved restart files. For example, if <code>restart_freq: 5</code>, we keep the restart files for every fifth run (restart004, restart009, restart014, etc.). Intermediate restarts are not deleted until a permanently archived restart has been produced. For example, if we have just completed run 11, then we keep restart004, restart009, restart010, and restart011. Restarts 10 through 13 are not deleted until restart014 has been saved.          <code>restart_freq: 1</code> saves all restart files.     </li> <li> mpirun arguments <pre><code>mpirun: --mca io ompio --mca io_ompio_num_aggregators 1</code></pre>         Append mpirun arguments to the mpirun call of the model.     </li> <li> qsub flags <pre><code>qsub_flags: -W umask=027</code></pre>         Configuration marker for any additional qsub flags.     </li> <li> Environment variables <pre><code>env:\n            \u00a0\u00a0UCX_LOG_LEVEL: 'error'\n        </code></pre>         Add the specified variables to the run environment.     </li> <li> Platform specific defaults <pre><code>platform: \n            \u00a0\u00a0nodesize: 48\n        </code></pre>         Set platform specific defaults.          nodesize sets the default number of cpus per node to fully utilise nodes regardless of requested number of cpus.     </li> <li> User scripts <pre><code>userscripts:\n            \u00a0\u00a0error: resub.sh\n            \u00a0\u00a0run: rm -f resubmit.count\n        </code></pre>         Namelist to include separate userscripts or subcommands at various stages of a payu submission.          <code>error</code> gets called if the model does not run correctly and returns an error code;          <code>run</code> gets called after model execution but prior to model output archive.     </li> </ul> <p> To know more about other configuration settings for the <code>config.yaml</code> file, please check how to configure your experiment with payu.</p>"},{"location":"models/run-a-model/run-access-om/#change-run-length","title":"Change run length","text":"<p>To change the internal run length, edit the <code>restart_period</code> field in the <code>&amp;date_manager_nml</code> section of the <code>~/access-om/1deg_jra55_iaf/accessom2.nml</code> file:</p> <pre><code>&amp;date_manager_nml\n    \u00a0\u00a0forcing_start_date = '1958-01-01T00:00:00'\n    \u00a0\u00a0forcing_end_date = '2019-01-01T00:00:00'\n    \u00a0\u00a0! Runtime for a single segment/job/submit, format is years, months, seconds,\n    \u00a0\u00a0! two of which must be zero.\n    \u00a0\u00a0restart_period = 5, 0, 0\n</code></pre>      The internal run length (controlled by <code>restart_period</code>) can be different from the total run length. Also, the <code>restart_period</code> value can be lowered, but should not be increased to a total of more than 5 years, to avoid errors. If you want to know more about the difference between internal run and total run lenghts, or if you want to run the model for more than 1 year, check Run configuration for multiple years."},{"location":"models/run-a-model/run-access-om/#run-access-om-configuration","title":"Run ACCESS-OM configuration","text":"<p>After editing the configuration, we are ready to run ACCESS-OM.   ACCESS-OM suites run on Gadi through a PBS job submission managed by payu.</p>"},{"location":"models/run-a-model/run-access-om/#payu-setup-optional","title":"Payu setup (optional)","text":"<p>As a first step, from the control directory, is good practice to run:</p> <pre><code>payu setup</code></pre> <p>This will prepare the model run, based on the experiment configuration.  payu setup laboratory path:  /scratch/$PROJECT/$USER/access-om2 binary ppath:  /scratch/$PROJECT/$USER/access-om2/bin input path:  /scratch/$PROJECT/$USER/access-om2/input work path:  /scratch/$PROJECT/$USER/access-om2/work archive path:  /scratch/$PROJECT/$USER/access-om2/archive Loading input manifest: manifests/input.yaml Loading restart manifest: manifests/restart.yaml Loading exe manifest: manifests/exe.yaml Setting up atmosphere Setting up ocean Setting up ice Setting up access-om2 Checking exe and input manifests Updating full hashes for 3 files in manifests/exe.yaml Creating restart manifest Writing manifests/restart.yaml Writing manifests/exe.yaml </p>      You can skip this step as it is included also in the run command. However, runnning it explicitly helps to check for errors and make sure executable and restart directories are accessible."},{"location":"models/run-a-model/run-access-om/#run-configuration","title":"Run configuration","text":"<p>To run ACCESS-OM configuration for one internal run length (controlled by <code>restart_period</code> in the <code>config.yaml</code> file), run:</p> <pre><code>payu run -f</code></pre> <p>This will submit a single job to the queue with a total run length of <code>restart_period</code>. </p>      The <code>-f</code> option ensures that payu will run even if there is an existing non-empty work directory, which happens if a run crashes.  <p> payu run -f payu: warning: Job request includes 47 unused CPUs. payu: warning: CPU request increased from 241 to 288 Loading input manifest: manifests/input.yaml Loading restart manifest: manifests/restart.yaml Loading exe manifest: manifests/exe.yaml payu: Found modules in /opt/Modules/v4.3.0 qsub -q normal -P tm70 -l walltime=10800 -l ncpus=288 -l mem=1000GB -N 1deg_jra55_iaf -l wd -j n -v PYTHONPATH=/g/data3/tm70/dm5220/scripts/python_modules/,PAYU_PATH=/g/data/hh5/public/apps/miniconda3/envs/analysis3-23.01/bin,PAYU_FORCE=True,MODULESHOME=/opt/Modules/v4.3.0,MODULES_CMD=/opt/Modules/v4.3.0/libexec/modulecmd.tcl,MODULEPATH=/g/data3/hh5/public/modules:/etc/scl/modulefiles:/opt/Modules/modulefiles:/opt/Modules/v4.3.0/modulefiles:/apps/Modules/modulefiles -W umask=027 -l storage=gdata/hh5+gdata/ik11+gdata/qv56 -- /g/data/hh5/public/apps/miniconda3/envs/analysis3-23.01/bin/python3.9 /g/data/hh5/public/apps/miniconda3/envs/analysis3-23.01/bin/payu-run &lt;job-ID&gt;.gadi-pbs </p>"},{"location":"models/run-a-model/run-access-om/#run-configuration-for-multiple-years","title":"Run configuration for multiple years","text":"<p>If you want to run ACCESS-OM configuration for multiple internal run lengths (controlled by <code>restart_period</code> in the <code>config.yaml</code> file), you can use the option <code>-n</code>:</p> <pre><code>payu run -f -n &lt;number-of-runs&gt;</code></pre> <p>This will run the configuration <code>number-of-runs</code> times with a total run length of <code>restart_period * number-of-runs</code>.  For example, if you want to run the configuration for a total of 50 years and you set <code>restart_period = 5, 0, 0</code> (5 years), you will have to set the <code>number-of-runs</code> to 10:</p> <pre><code>payu run -f -n 10</code></pre>"},{"location":"models/run-a-model/run-access-om/#monitor-access-om-runs","title":"Monitor ACCESS-OM runs","text":"<p>Currently, there is no specific tool to monitor ACCESS-OM runs.   One way to check the status of our run is running:</p> <pre><code>qstat -u $USER</code></pre> <p>This will show the status of all your PBS jobs (if there is any PBS job submitted):  qstat -u $USER Job id\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Name\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0User\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Time Use\u00a0S Queue ---------------------  ---------------- ----------------  -------- - ----- &lt;job-ID&gt;.gadi-pbs\u00a0\u00a0\u00a0\u00a0\u00a01deg_jra55_iaf\u00a0\u00a0\u00a0&lt;$USER&gt;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&lt;time&gt;\u00a0R\u00a0normal-exec &lt;job-ID-2&gt;.gadi-pbs\u00a0\u00a0\u00a0&lt;other-job-name&gt;\u00a0&lt;$USER&gt;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&lt;time&gt;\u00a0R\u00a0normal-exec &lt;job-ID-3&gt;.gadi-pbs\u00a0\u00a0\u00a0&lt;other-job-name&gt;\u00a0&lt;$USER&gt;\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&lt;time&gt;\u00a0R\u00a0normal-exec  If you changed the <code>jobname</code> in the PBS resources of the Master Configuration file, that will be your job's Name instead of <code>1deg_jra55_iaf</code>.  S indicates the status of your run:</p> <ul> <li>Q \u2192 Job waiting in the queue to start</li> <li>R \u2192 Job running</li> <li>E \u2192 Job ending</li> </ul> <p>If there is no listed job with your <code>jobname</code> (or if there is no job submitted at all), your run might have successfully completed, or might have been terminated due to an error.</p>"},{"location":"models/run-a-model/run-access-om/#stop-a-run","title":"Stop a run","text":"<p>If you want to manually terminate a run, you can do it by running:</p> <pre><code>qdel &lt;job-ID&gt;</code></pre>"},{"location":"models/run-a-model/run-access-om/#error-and-output-log-files","title":"Error and output log files","text":"<p>While the model is running, payu saves the standard output and standard error into the files <code>access-om2.out</code> and <code>access-om2.err</code> in the control directory. You can examine these files, as the run progresses, to check on it's status.  After the model has completed its run, or if it crashed, the output and error log files, respectively, are renamed by default into <code>jobname.o&lt;job-ID&gt;</code> and <code>jobname.e&lt;job-ID&gt;</code>.</p>"},{"location":"models/run-a-model/run-access-om/#access-om-outputs","title":"ACCESS-OM outputs","text":"<p>While the configuration is running, output files (as well as restart files) are moved from the <code>work</code> directory to the <code>archive</code> directory, under <code>/scratch/$PROJECT/$USER/access-om2/archive</code> (also symlinked in the control directory under <code>~/access-om/1deg_jra55_iaf/archive</code>).  Both outputs and restarts are stored into subfolders for each different configuration (<code>1deg_jra55_iaf</code> in our case), and inside the configuration folder, they are subdivided for each internal run.  The format of a typical output folder is <code>outputXXX</code>, whereas the typical restart folder is usually formatted as <code>restartXXX</code>, with XXX being the number of internal run, starting from <code>000</code>.  In the respective folders, outputs and restarts are separated for each model component.  cd /scratch/$PROJECT/$USER/access-om2/archive/1deg_jra55_iaf ls output000 pbs_logs restart000 </p> <p></p> References <ul> <li> http://www.cosima.org.au </li> <li> Kiss et al. (2020) </li> <li> https://github.com/COSIMA/access-om2 </li> <li> https://github.com/COSIMA/access-om2/wiki/Getting-started#quick-start </li> <li> https://payu.readthedocs.io/en/latest/usage.html </li> </ul>"},{"location":"models/run-a-model/getting_started/","title":"Getting Started to Run a Model","text":"<p>If Model, Model Component or Model Configuration are not familiar terms for you, please check out our Model overview.</p> <p>If you have not run a model before, our Getting Started Guide will give you the basics to access the Model infrastructure on the high-performance-computing facility Gadi@NCI.</p> <p>Detailed guides for the different Model configurations can then be found on the following pages: -  Run ACCESS-ESM for the ACCESS Earth System Model configurations -  Run ACCESS-CM for the ACCESS Coupled Model configurations -  Run ACCESS-AM for the ACCESS Atmosphere Model configurations -  Run ACCESS-OM for the ACCESS Ocean Model configurations  </p>"},{"location":"models/run-a-model/getting_started/access_to_gadi_at_nci/","title":"Getting Started: Computing Access (Gadi@NCI)","text":"<p>Here, we provide you the important information to give you access to the large data that we curate at NCI's storage:</p> <p>1) Get an NCI Account 2) Join relevant NCI projects 3) Logging in to Gadi@NCI 4) Computing on Gadi</p>"},{"location":"models/run-a-model/getting_started/access_to_gadi_at_nci/#1-nci-account","title":"1) NCI Account","text":"<p>To be able to work with our data, you need an NCI account.</p> <p>If you don't have one yet, signup here.</p> <p>Note: You will need an institutional email address with an organisation that allows access to NCI (e.g., CSIRO, a university, etc.).</p> <p>Once you have signed up, you will be allocated a username. We will refer to this username (e.g. <code>kf1234</code>) as <code>$USER</code>.</p>"},{"location":"models/run-a-model/getting_started/access_to_gadi_at_nci/#2-join-relevant-nci-projects","title":"2) Join relevant NCI projects","text":"<p>There is a plethora of NCI projects that may or may not be relevant for you.</p> <p>We recommend you have a chat with your supervisor to identify the relevant projects, but in any case suggest to join <code>xp65</code> for MED code as well as <code>kj13</code> for MED data.</p> <p>To get this conversation started, we list some possibly relevant projects below:</p> Project Description with link, * indicated compute resource ACCESS-NRI projects tm70 ACCESS-NRI Working Project * iq82 ACCESS-NRI MED Compute * kj13 ACCESS-NRI MED Data Dev ct11 ACCESS-NRI Replicated Datasets xp65 ACCESS-NRI Analysis Environments ACCESS projects access ACCESS software sharing p66 ACCESS - AOGCM / suppport development of the ACCESS modelling system * p73 ACCESS Model Output Archive (AOGCM) Data projects hh5 Climate-LIEF Data Storage ub7 Seasonal Prediction ACCESS-S1 Hindcast ux62 Seasonal Prediction ACCESS-S2 Hindcast cb20 ESGF CMIP3 Replication Data al33 ESGF CMIP5 Replication Data rr3 ESGF CMIP5 Australian Data Publication oi10 ESGF CMIP6 Replication Data fs38 ESGF CMIP6 Australian Data Publication rt52 ERA5 Replicated Data: Single and pressure-levels data uc16 ERA5 Replicated Datasets on Potential Temperature &amp; Vorticity Levels zz93 ERA5-Land Replicated Data zv2 Australian Gridded Climate Data (AGCD) Collection qv56 Reference Datasets for Climate Model Analysis/Forcing cj50 COSIMA Model Output Collection Other projects ik11 COSIMA shared working space v45 Ocean Extremes * ga6 Modelling the formation of sedimentary basins and continental margins * m18 Evolution and dynamics of the Australian lithosphere * q97 Earth dynamics and resources over the last billion years * qu79 Collaborative REAnalysis Technical Environment Intercomparison Project (CREATE-IP) <p>To join a project or find more projects, please use this NCI website.</p> <p>The first project that you join will become your default login project, e.g. <code>xp65</code>. We will refer to it as <code>$PROJECT</code> and we show you how to change it below.</p>"},{"location":"models/run-a-model/getting_started/access_to_gadi_at_nci/#3-logging-in-to-gadinci","title":"3) Logging in to Gadi@NCI","text":"<p>If you have never logged onto Gadi before, we recommend to take a look at NCI's Welcome to Gadi website. It provides all the important commands and information for logging properly onto Gadi, like the following: \"To run jobs on Gadi, you need to first log in to the system. Users on Mac/Linux can use the built-in terminal. For Windows users, we recommend using MobaXterm as the local terminal. Logging in to Gadi happens through a Gadi login node.\"</p> <p>When you login, via the command <pre><code>ssh -Y $USER@gadi.nci.org.au\n</code></pre> you will enter your $HOME directory with your default <code>$PROJECT</code> and your default SHELL. Both are saved at <code>$HOME/.config/gadi-login.conf</code> and you can print them via <pre><code>cat $HOME/.config/gadi-login.conf\n</code></pre></p> <p>The <code>-Y</code> option is needed to run graphical tools by enabling the forwarding of trusted X protocol mesgs between X-Server on local system and X programs on Gadi.  You need to enable X Windowing system on your local system before running ssh. This can be done by running X-Server like XQuartz (Mac), MobaXterm (MS Windows), startx or similar (Linux).</p> <p>Again, for more useful information we recommend to check out NCI's Welcome to Gadi website.</p>"},{"location":"models/run-a-model/getting_started/access_to_gadi_at_nci/#4-computing-on-gadi","title":"4) Computing on Gadi","text":""},{"location":"models/run-a-model/getting_started/access_to_gadi_at_nci/#gadi-resources","title":"Gadi Resources","text":"<p>Coupled climate models like ACCESS-CM involve, among other things, calculation of complex mathematical equations that explain the physics of the atmosphere and oceans. Performed at hundreds of millions of points around the Earth, these calculations require vast computing power to complete them in a reasonable amount of time, thus relying on the power of  high-performance computing (HPC) like Gadi. The Gadi supercomputer can handle more than 10 million billion (10 quadrillion) calculations per second and is connected to 100,000 Terabytes of high-performance research data storage.</p> <p>An overview of Gadi resources such as compute, storage and PBS jobs are described below. </p> <p>Useful NCI commands to check your available compute resources are:</p> Command Purpose <code>logout</code> or Ctrl+D To exit a session <code>hostname</code> Displays login node details <code>module list</code> Modules currently loaded <code>module avail</code> Available modules <code>nci_account -P [proj]</code> Compute allocation for [proj] <code>nqstat -P [proj]</code> Jobs running/queued in [proj] <code>lquota</code> Storage allocation and usage for all your projects"},{"location":"models/run-a-model/getting_started/access_to_gadi_at_nci/#compute-hours","title":"Compute Hours","text":"<p>Compute allocations are granted to projects instead of directly to users and, hence, you need to be a member of a project in order to use its compute allocation. To run jobs on Gadi, you need to have sufficient allocated compute hours available, where the job cost depends on the resources reserved for the job and the amount of walltime it uses. </p>"},{"location":"models/run-a-model/getting_started/access_to_gadi_at_nci/#storage","title":"Storage","text":"<p>Each user has a project-independent <code>$HOME</code> directory, which has a storage limit of 10 GiB. All data on <code>/home</code> is backed up.</p> <p>Through project membership, the user gets access to the storage space within the project folders <code>/scratch</code> and  <code>/g/data</code> filesystems for that particular project.</p>"},{"location":"models/run-a-model/getting_started/access_to_gadi_at_nci/#pbs-jobs","title":"PBS Jobs","text":"<p>To run compute tasks such as an ACCESS-CM suite on Gadi, users need to submit them as jobs to queues. Within a job submission, you can specify the queue, duration and computational resources needed for your job. When a job submission is accepted, it is assigned a jobID (shown in the return message) that can then be used to monitor the job\u2019s status. </p> <p>On job completion, contents of the job\u2019s standard output/error stream gets copied to a file in the working directory with the respective format: <code>&lt;jobname&gt;.o&lt;jobid&gt;</code> and <code>&lt;jobname&gt;.e&lt;jobid&gt;</code>. Users should check these two log files before proceeding with post-processing of any output from their corresponding job.</p>"}]}